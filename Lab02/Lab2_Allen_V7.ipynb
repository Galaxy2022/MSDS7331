{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining 7331 - Fall 2019\n",
    "## Lab 2 - Classification\n",
    "\n",
    "* **Allen Ansari**\n",
    "* **Chad Madding**\n",
    "* **Yongjun (Ian) Chu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Cardiovascular diseases (CVD) are the no. 1 cause of death in US each year. To reduce the death rate, the best approach is by early detection and screening. In this Lab we will implement models including decision tree, Random Forest, KNN and Support Vector Machine (SVM) to predict the probability of a patient having CVD based on results from medical examinations, such as blood pressure values and glucose content.\n",
    "In addition, we will also try to set up models to predict cholesterol level (normal or not) for each patient based on their medical exam results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description\n",
    "\n",
    "We will be performing an analysis of the cardiovascular diseases dataset found on Kaggle (https://www.kaggle.com/sulianova/cardiovascular-disease-dataset). It consists of 70,000 records of patient’s data in 12 features, such as age, gender, systolic blood pressure, diastolic blood pressure and CVD status(binary, 1 or 0).\n",
    "There are 3 types of input features:\n",
    "\n",
    "- *Objective*: factual information;\n",
    "- *Examination*: results of medical examination;\n",
    "- *Subjective*: information given by the patient.\n",
    "\n",
    "|Feature   |Variable Type   |Variable   |Value Type   |\n",
    "|:---------|:--------------|:---------------|:------------|\n",
    "| Age | Objective Feature | age | int (days) |\n",
    "| Height | Objective Feature | height | int (cm) |\n",
    "| Weight | Objective Feature | weight | float (kg) |\n",
    "| Gender | Objective Feature | gender | categorical code |\n",
    "| Systolic blood pressure | Examination Feature | ap_hi | int |\n",
    "| Diastolic blood pressure | Examination Feature | ap_lo | int |\n",
    "| Cholesterol | Examination Feature | cholesterol | 1: normal, 2: above normal, 3: well above normal |\n",
    "| Glucose | Examination Feature | gluc | 1: normal, 2: above normal, 3: well above normal |\n",
    "| Smoking | Subjective Feature | smoke | binary |\n",
    "| Alcohol intake | Subjective Feature | alco | binary |\n",
    "| Physical activity | Subjective Feature | active | binary |\n",
    "| Presence or absence of cardiovascular disease | Target Variable | cardio | binary |\n",
    "\n",
    "For any binary data type, \"0\" means \"No\" and \"1\" means \"Yes\". All of the dataset values were collected at the moment of medical examination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents<a id=\"top\"></a>\n",
    "\n",
    "* **[Data Preparation Part 1](#Data_Preparation_Part_1)**\n",
    "    *  Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "* **[Data Preparation Part 2](#Data_Preparation_Part_2)**\n",
    "    *  Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "* **[Modeling and Evaluation 1](#Modeling_and_Evaluation_1)**\n",
    "    *  Choose and explain your evaluation metrics that you will use (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.\n",
    "* **[Modeling and Evaluation 2](#Modeling_and_Evaluation_2)**\n",
    "    *  Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "* **[Task 1](#Task_1)**\n",
    "    * **[1.1 Modeling and Evaluation 3](#1.1_Modeling_and_Evaluation_3)**\n",
    "         *  Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "    * **[1.2 Modeling and Evaluation 4](#1.2_Modeling_and_Evaluation_4)**\n",
    "         *  Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "    * **[1.3 Modeling and Evaluation 5](#1.3_Modeling_and_Evaluation_5)**\n",
    "         *  Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
    "    * **[1.4 Modeling and Evaluation 6](#1.4_Modeling_and_Evaluation_6)**\n",
    "         *  Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n",
    "* **[Task 2](#Task_2)**    \n",
    "    * **[2.1 Modeling and Evaluation 3](#2.1_Modeling_and_Evaluation_3)**\n",
    "         *  Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "    * **[2.2 Modeling and Evaluation 4](#2.2_Modeling_and_Evaluation_4)**\n",
    "         *  Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "    * **[2.3 Modeling and Evaluation 5](#2.3_Modeling_and_Evaluation_5)**\n",
    "         *  Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
    "    * **[2.4 Modeling and Evaluation 6](#2.4_Modeling_and_Evaluation_6)**\n",
    "         *  Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n",
    "* **[Deployment](#Deployment)**\n",
    "    *  How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would you deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?\n",
    "* **[Exceptional Work](#Exceptional_Work)**\n",
    "    *  You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "### Data_Preparation_Part_1 <a id=\"Data_Preparation_Part_1\"></a>\n",
    "* We obtained this CVD dataset from Kaggle. The purpose of this dataset was to determine which medical aspects had the most bearing on whether a patient would have CVD or not and have normal or not normal cholesterol level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\04616598\\.conda\\envs\\ML7331\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0   0  18393       2     168    62.0    110     80            1     1      0   \n",
       "1   1  20228       1     156    85.0    140     90            3     1      0   \n",
       "2   2  18857       1     165    64.0    130     70            3     1      0   \n",
       "3   3  17623       2     169    82.0    150    100            1     1      0   \n",
       "4   4  17474       1     156    56.0    100     60            1     1      0   \n",
       "\n",
       "   alco  active  cardio  \n",
       "0     0       1       0  \n",
       "1     0       1       1  \n",
       "2     0       0       1  \n",
       "3     0       1       1  \n",
       "4     0       0       0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import data all necessary libraries we will be using in our estimation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import re\n",
    "import sklearn\n",
    "import statistics\n",
    "import random\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectPercentile, RFE, SelectFromModel\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Binarizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, auc, roc_curve\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, TimeSeriesSplit, StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor,AdaBoostClassifier,RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Bring in data set\n",
    "df = pd.read_csv('data/cardio_train.csv', sep= ';') #read in the csv file\n",
    "\n",
    "# Show the dimension and the first 5 rows of the dataset\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our data preparation includes following steps:\n",
    "* Remove the \"id\" attribute as it is not needed in this assignment\n",
    "* Inspect each feature for NA values. If more than 75% of the feature contains NA, we replace that field with 0. If less than 75% is NA, then the median value of the column is used to replace the NA\n",
    "* Check for any categorical variables and using proper methods (like one-hot) to convert them to numerical variables\n",
    "* Remove all duplicate entries in the dataset\n",
    "* Search each feature for any outliers and remove them from dataset. We will keep the entries between 97.5% quantile and 2.5% quantile for those features which have outliers or incorrect data \n",
    "* Add new variable call Body mass index (BMI) which is commonly used in medical field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'id' column \n",
    "if 'id' in df:\n",
    "    del df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age            0\n",
       "gender         0\n",
       "height         0\n",
       "weight         0\n",
       "ap_hi          0\n",
       "ap_lo          0\n",
       "cholesterol    0\n",
       "gluc           0\n",
       "smoke          0\n",
       "alco           0\n",
       "active         0\n",
       "cardio         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total missing values for each feature\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see the data type of each column of the CVD dataset and see if there are any categorical variables in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no missing values or NAs in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# are there any NA values in the dataset\n",
    "if df.isnull().values.any():\n",
    "    print('There are NAs or missing values in the datasets.')\n",
    "else:\n",
    "    print('There are no missing values or NAs in the dataset.')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "gender\n",
      "height\n",
      "weight\n",
      "ap_hi\n",
      "ap_lo\n",
      "cholesterol\n",
      "gluc\n",
      "smoke\n",
      "alco\n",
      "active\n",
      "cardio\n",
      "70000\n"
     ]
    }
   ],
   "source": [
    "# Are there any non-numbers in each column?\n",
    "for column in df:\n",
    "    print(column)\n",
    "    cnt=0\n",
    "    for row in df[column]:\n",
    "        try:\n",
    "            float(row)\n",
    "        except ValueError:\n",
    "            print(\"there is a non-numeric value: \" + row)\n",
    "            pass\n",
    "        cnt+=1\n",
    "        \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All values in the dataset are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows except first occurrence based on all columns are :\n",
      "         age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
      "10562  20495       1     165    70.0    120     80            1     1      0   \n",
      "21784  16793       1     165    68.0    120     80            1     1      0   \n",
      "38505  18988       1     164    65.0    120     80            1     1      0   \n",
      "40365  14552       1     158    64.0    120     80            1     1      0   \n",
      "42450  18353       1     169    67.0    120     80            1     1      0   \n",
      "44653  16937       2     170    70.0    120     80            1     1      0   \n",
      "45125  21280       1     165    65.0    120     80            1     1      0   \n",
      "45748  22077       1     175    69.0    120     80            1     1      0   \n",
      "45810  21230       1     164    62.0    120     80            1     1      0   \n",
      "48917  21945       1     165    60.0    120     80            1     1      0   \n",
      "50432  17493       2     169    74.0    120     80            1     1      0   \n",
      "52552  21943       1     165    65.0    120     80            1     1      0   \n",
      "56643  17535       2     165    65.0    120     80            1     1      0   \n",
      "56906  20293       1     162    70.0    110     70            1     1      0   \n",
      "57946  18955       1     165    75.0    120     80            1     1      0   \n",
      "58730  19858       1     165    68.0    120     80            1     1      0   \n",
      "60453  20516       1     164    66.0    120     80            1     1      0   \n",
      "60474  16805       1     157    67.0    120     80            1     1      0   \n",
      "62318  18979       1     165    65.0    120     80            1     1      0   \n",
      "64169  16160       1     168    65.0    120     80            1     1      0   \n",
      "65079  18210       1     160    60.0    120     80            1     1      0   \n",
      "65622  21778       1     160    58.0    120     80            1     1      0   \n",
      "66190  19059       1     165    65.0    120     80            1     1      0   \n",
      "68281  21119       1     160    60.0    120     80            1     1      0   \n",
      "\n",
      "       alco  active  cardio  \n",
      "10562     0       1       0  \n",
      "21784     0       1       0  \n",
      "38505     0       1       0  \n",
      "40365     0       1       0  \n",
      "42450     0       1       0  \n",
      "44653     0       0       0  \n",
      "45125     0       1       0  \n",
      "45748     0       1       1  \n",
      "45810     0       1       0  \n",
      "48917     0       1       0  \n",
      "50432     0       1       1  \n",
      "52552     0       1       1  \n",
      "56643     0       1       0  \n",
      "56906     0       1       0  \n",
      "57946     0       1       1  \n",
      "58730     0       1       0  \n",
      "60453     0       0       0  \n",
      "60474     0       1       0  \n",
      "62318     0       0       0  \n",
      "64169     0       1       1  \n",
      "65079     0       1       0  \n",
      "65622     0       1       0  \n",
      "66190     0       1       1  \n",
      "68281     0       0       1  \n",
      "\n",
      "There are 24 duplicated entries in the dataset!\n"
     ]
    }
   ],
   "source": [
    "#Are there any duplicate entries in the dataset?\n",
    "duplicateRowsDF = df[df.duplicated(keep='first')]\n",
    "\n",
    "print(\"Duplicate Rows except first occurrence based on all columns are :\")\n",
    "print(duplicateRowsDF)\n",
    "\n",
    "print(f\"\\nThere are {len(duplicateRowsDF)} duplicated entries in the dataset!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For duplicated entries, we think they were just from mistakes by entering more than once. These 24 entries should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69976, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove duplicates in the dataframe\n",
    "df.drop_duplicates(keep = 'first', inplace = True) \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>69976.000000</td>\n",
       "      <td>69976.000000</td>\n",
       "      <td>69976.000000</td>\n",
       "      <td>69976.000000</td>\n",
       "      <td>69976.000000</td>\n",
       "      <td>69976.000000</td>\n",
       "      <td>69976.000000</td>\n",
       "      <td>69976.000000</td>\n",
       "      <td>69976.000000</td>\n",
       "      <td>69976.000000</td>\n",
       "      <td>69976.000000</td>\n",
       "      <td>69976.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>19468.950126</td>\n",
       "      <td>1.349648</td>\n",
       "      <td>164.359152</td>\n",
       "      <td>74.208519</td>\n",
       "      <td>128.820453</td>\n",
       "      <td>96.636261</td>\n",
       "      <td>1.366997</td>\n",
       "      <td>1.226535</td>\n",
       "      <td>0.088159</td>\n",
       "      <td>0.053790</td>\n",
       "      <td>0.803718</td>\n",
       "      <td>0.499771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2467.374620</td>\n",
       "      <td>0.476862</td>\n",
       "      <td>8.211218</td>\n",
       "      <td>14.397211</td>\n",
       "      <td>154.037729</td>\n",
       "      <td>188.504581</td>\n",
       "      <td>0.680333</td>\n",
       "      <td>0.572353</td>\n",
       "      <td>0.283528</td>\n",
       "      <td>0.225604</td>\n",
       "      <td>0.397187</td>\n",
       "      <td>0.500004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>10798.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>-150.000000</td>\n",
       "      <td>-70.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>17664.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>19703.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>21327.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>23713.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>16020.000000</td>\n",
       "      <td>11000.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        gender        height        weight         ap_hi  \\\n",
       "count  69976.000000  69976.000000  69976.000000  69976.000000  69976.000000   \n",
       "mean   19468.950126      1.349648    164.359152     74.208519    128.820453   \n",
       "std     2467.374620      0.476862      8.211218     14.397211    154.037729   \n",
       "min    10798.000000      1.000000     55.000000     10.000000   -150.000000   \n",
       "25%    17664.000000      1.000000    159.000000     65.000000    120.000000   \n",
       "50%    19703.000000      1.000000    165.000000     72.000000    120.000000   \n",
       "75%    21327.000000      2.000000    170.000000     82.000000    140.000000   \n",
       "max    23713.000000      2.000000    250.000000    200.000000  16020.000000   \n",
       "\n",
       "              ap_lo   cholesterol          gluc         smoke          alco  \\\n",
       "count  69976.000000  69976.000000  69976.000000  69976.000000  69976.000000   \n",
       "mean      96.636261      1.366997      1.226535      0.088159      0.053790   \n",
       "std      188.504581      0.680333      0.572353      0.283528      0.225604   \n",
       "min      -70.000000      1.000000      1.000000      0.000000      0.000000   \n",
       "25%       80.000000      1.000000      1.000000      0.000000      0.000000   \n",
       "50%       80.000000      1.000000      1.000000      0.000000      0.000000   \n",
       "75%       90.000000      2.000000      1.000000      0.000000      0.000000   \n",
       "max    11000.000000      3.000000      3.000000      1.000000      1.000000   \n",
       "\n",
       "             active        cardio  \n",
       "count  69976.000000  69976.000000  \n",
       "mean       0.803718      0.499771  \n",
       "std        0.397187      0.500004  \n",
       "min        0.000000      0.000000  \n",
       "25%        1.000000      0.000000  \n",
       "50%        1.000000      0.000000  \n",
       "75%        1.000000      1.000000  \n",
       "max        1.000000      1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAANTCAYAAADv0v9PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3X1sXudhHvyLEklVcuKPjI4tS1ki733s5WMbhgZcMa7zW8IBbCXy2HZ4olqRVyOzFSAp3dYsJICki1lUIQE0mrAtWjnFmkmRYx9sy4OoU9HXjrFkL7uVzd51XRLP4hAnsRjVNh3JskxbpCW+fyRSzJiyRfHj8Ei/3z8Pn1vk0eUblMTL933u0zQ9PR0AAACWvxVlBwAAAODCKHAAAAAVocABAABUhAIHAABQEQocAABARShwAAAAFaHAAQAAVIQCBwAAUBEKHAAAQEU0lx3gx6bLDgAAAFCyprf7hOVS4PKDH/yg7AhJkra2toyPj5cdY9kxL29mTmZnXmZnXmZnXt7MnMzOvMzOvMzOvLyZOZndcpqXG2644YI+zxZKAACAilDgAAAAKkKBAwAAqAgFDgAAoCIUOAAAgIpQ4AAAACpCgQMAAKgIBQ4AAKAiFDgAAICKUOAAAAAqQoEDAACoCAUOAACgIhQ4AACAilDgAAAAKkKBAwAAqAgFDgAAoCIUOAAAgIpQ4AAAACqiuczfvF6vb0qyqSiKMmMAAABUQqkFriiKg0kOJrmnzBwAAABVYAslAABARShwAAAAFaHAAQAAVIQCBwAAUBEKHAAAQEUocAAAABWhwAEAAFSEAgcAAFARChwAAEBFKHAAAAAVocABAABUhAIHAABQEQocAABARShwAAAAFaHAAQAAVIQCBwAAUBHNZQdYSuvWrVuwa42NjS3YtQAAAC7EZVXgLqR0nb7njqz8/FeWIA0AAMDc2EIJAABQEQocAABARShwAAAAFaHAAQAAVIQCBwAAUBGXzCmUp++7M5k4uTDXuueO+V1gzTuy8nOPLEgWAACAsy6ZApeJkwty/H9bW1vGx8fndY15F0AAAIBZ2EIJAABQEQocAABARShwAAAAFVHqPXD1en1Tkk1FUZQZAwAAoBJKLXBFURxMcjDJPfO91qFb9yWPHZ9/qCzANW7dl03zvwoAAMAMl8wplBufuGt5nUL58flnAQAAeCP3wAEAAFSEAgcAAFARChwAAEBFKHAAAAAVocABAABUhAIHAABQEZfMYwSSHx/fP0/PLUCOrHnHQlwFAABghkumwC3EM+CSH5XAhboWAADAQrKFEgAAoCIUOAAAgIpQ4AAWUaPRSGdnZ1avXp3Ozs40Go2yIwEAFXbJ3AMHsNw0Go3s2bMng4OD2bhxYw4dOpSenp4kSVdXV8npAIAqsgIHsEiGhoYyODiYjo6OtLS0pKOjI4ODgxkaGio7GgBQUQocwCIZHR1Ne3v7jLH29vaMjo6WlAgAqDoFDmCR1Gq1jIyMzBgbGRlJrVYrKREAUHUKHMAi6e7uTk9PT4aHhzM1NZXh4eH09PSku7u77GgAQEU5xARgkZw9qKS/vz+bN29OrVbL9u3bHWACAFw0BQ5gEXV1daWrqyttbW0ZHx8vOw4AUHG2UAIAAFSEAgcAAFARChzAIrr11luzbt26rFq1KuvWrcutt95adiQAoMIUOIBFcuutt+app57KRz7ykYyNjeUjH/lInnrqKSUOALhol9UhJuvWrbvQT3zbTxkbG5tnGuBSd7a8feELX0hbW1u+8IUv5Fd/9Vfz+OOPlx0NAKioy6rAXUjpclIcsJAGBwff9P4f/aN/VFIaAKDqbKEEWEQ9PT1v+R4AYC4UOIBF8v73vz+PP/54fvVXfzXj4+Pntk++//3vLzsaAFBRl9UWSoCl9MQTT+SDH/xgHn/88XP34F599dV54oknSk4GAFSVFTiARdLX15eXX345DzzwQI4dO5YHHnggL7/8cvr6+sqOBgBUlAIHsEgOHDiQ3t7ebNu2LWvWrMm2bdvS29ubAwcOlB0NAKgoBQ5gkUxOTmbr1q0zxrZu3ZrJycmSEgEAVafAASyS1tbW7N+/f8bY/v3709raWlIiAKDqFDiARbJly5bs2rUre/fuzcTERPbu3Ztdu3Zly5YtZUcDACrKKZQAi2RgYCBJsnv37jz44INpbW3NXXfddW4cAGCuFDiARTQwMJCBgYG0tbVlfHy87DgAQMXZQgkAAFARChzAImo0Guns7Mzq1avT2dmZRqNRdiQAoMJsoQRYJI1GI3v27Mng4GA2btyYQ4cOpaenJ0nS1dVVcjoAoIqswAEskqGhoQwODqajoyMtLS3p6OjI4OBghoaGyo4GAFSUAvdjtjkBC210dDRHjx6d8XfL0aNHMzo6Wna00vk7FwAuji2U+dEPEvfff39ee+21JMnTTz+d+++/P4ltTsDFu+6667Jr1678/u///rktlJ/5zGdy3XXXlR2tVLaWAsDFswKX5Ld+67fy2muvZevWrXn++eezdevWvPbaa/mt3/qtsqMBXHJsLQWAi6fAJZmYmMiWLVuye/fuXHXVVdm9e3e2bNmSiYmJsqMBFfbcc8+lr68v/f39ufLKK9Pf35++vr4899xzZUcr1ejoaNrb22eMtbe321oKABdAgfuxW2+99S3fA8xVrVbL9ddfnyeffDKvvvpqnnzyyVx//fWp1WplRytVrVbLyMjIjLGRkZHLfl4A4EIocD/2a7/2axkeHs7U1FSGh4fza7/2a2VHAiquu7s7PT09M/5u6enpSXd3d9nRSmVeAODiLcohJvV6/YokX0/y20VR/Oli/B4L6ZZbbsnXvva13HPPPXn55Zfzzne+MydPnswtt9xSdjSgws4eyNHf35/NmzenVqtl+/btl/1BHeYFAC7eBRW4er3+b5N8LMnzRVF86A3jtyX5XJKVSf64KIrdP/6l7UmKBc66aB555JHceeed+frXv57p6emcOHEit9xySx555JGyowEV19XVla6urrS1tWV8fLzsOMtGURQ5fPhwpqenc/jw4RRFocABwAW40C2UX0hy2xsH6vX6yiR/kOT2JB9I8iv1ev0D9Xr91iTfTlKpu/QfeeSRHDlyJKdOncqRI0eUN4BFcuedd+ZrX/taPvGJT+T555/PJz7xiXzta1/LnXfeWXY0AFj2LqjAFUXx9SQ//Knh9iT/pyiK7xRFMZnk0ST/IskvJPm5JHcmuader7vPDoBzvv71r+fnf/7nMzIykuuvvz4jIyP5+Z//+Xz9618vOxoALHvzuQduXZJn3/D+SJJ/UhTFZ5KkXq//apLxoijOzPbF9Xr93iT3Jj/aStPW1jaPKAunubl52WRZTszLm5mT2ZmX2ZmXn5iens73vve9fP7zn59xD/L09LQ5iu+V8zEvszMvszMvb2ZOZlfFeZlPgWuaZWz67AdFUXzhrb64KIqHkzx89uuWy70h7lOZnXl5M3MyO/MyO/My080335wPfehDaWpqyoc+9KHcfPPN+f73v2+O4nvlfMzL7MzL7MzLm5mT2S2nebnhhhsu6PPms73xSJL3vOH9+iQ/mMf1ALhMPP7449mxY0deeuml7NixI48//njZkQCgEuazAvdXSWr1en1DkrEkm/Oj+94A4LxuvvnmrFq1Kl/84hezf//+NDU15R/+w3+YU6dOlR0NAJa9C1qBq9frX0ryX5PcXK/Xj9Tr9U8WRfF6ks8k+fMkTyUpiqL41uJFBeBS0N3dnbGxsaxfvz5NTU1Zv359xsbGPMgbAC7ABa3AFUXxK+cZP5Tk0IImAuCSd/Lkybz44otJkmeffTarVq0qOREAVIMj/gFYUr29vZmamsoDDzyQY8eO5YEHHsjU1FR6e3vLjgYAy9587oEDgDk7fvx4ent7s23btqxZsybbtm3L6dOns2vXrrKjAcCyV2qBq9frm5JsKoqizBgALLGbb775Ld8DALMrtcAVRXEwycEk95SZA4Cl09zcnE996lP5O3/n72RsbCzr1q3Liy++mOZmm0IA4O24Bw6AJdXR0ZGJiYmcOHEiZ86cyYkTJzIxMZGOjo6yowHAsqfAAbCk/vZv/za33XZbXn311STJq6++mttuuy1/+7d/W3IyAFj+FDgAltTo6Ghuv/32bNiwIStWrMiGDRty++23Z3R0tOxoALDsueEAgCV13XXX5b777jv3/umnn859992XtWvXlpgKAKrBChwAS+ro0aNzGgcAfkKBA6AU1157bZqamnLttdeWHQUAKkOBA2DJ3XHHHfnrv/7rvPbaa/nrv/7r3HHHHWVHAoBKKLXA1ev1TfV6/eEyMwCw9P70T/80w8PDmZqayvDwcP70T/+07EgAUAke5A3Akjtz5kzuvPPOnD59OitXrsyZM2fKjgQAlWALJQBL6u67706SvP7665mens7rr78+YxwAOD+PEQBgSQ0MDCRJDhw4kMnJybS2tmbLli3nxgGA81PgAFhyAwMDGRgYSFtbW8bHx8uOAwCVYQslAEuur68vGzZsyKpVq7Jhw4b09fWVHQkAKkGBA2BJ9fX1Zd++fdmxY0eOHTuWHTt2ZN++fUocAFwABQ6AJXXgwIH09vZm27ZtWbNmTbZt25be3t4cOHCg7GgAsOwpcAAsqcnJyVx99dXp7OzM6tWr09nZmauvvjqTk5NlRwOAZa/UQ0zq9fqmJJuKoigzBgBLaOXKlXnwwQfz8MMPZ+PGjTl06FDuvfferFy5suxoALDslboCVxTFwaIo7i0zAwBL653vfGdOnDiRb37zm5mamso3v/nNnDhxIu985zvLjgYAy54tlAAsqRMnTmTr1q3ZvXt3rrnmmuzevTtbt27NiRMnyo4GAMueAgfAkqrVavnoRz+aZ555JqdOncozzzyTj370o6nVamVHA4BlT4EDYEl1d3enp6cnw8PDmZqayvDwcHp6etLd3V12NABY9ko9xASAy09XV1eSpL+/P5s3b06tVsv27dvPjQMA56fAAbDkurq60tXVlba2toyPj5cdBwAqwxZKAACAilDgAAAAKkKBAwAAqAgFDgAAoCIUOAAAgIootcDV6/VN9Xr94TIzALD0+vr6smHDhqxatSobNmxIX19f2ZEAoBJKLXBFURwsiuLeMjMAsLT6+vqyb9++7NixI8eOHcuOHTuyb98+JQ4ALoAtlAAsqQMHDqS3tzfbtm3LmjVrsm3btvT29ubAgQNlRwOAZU+BA2BJTU5OZuvWrTPGtm7dmsnJyZISAUB1KHAALKnW1tbs379/xtj+/fvT2tpaUiIAqA4FDoAltWXLluzatSt79+7NxMRE9u7dm127dmXLli1lRwOAZa+57AAAXF4GBgaSJLt3786DDz6Y1tbW3HXXXefGAYDzU+AAWHIDAwMZGBhIW1tbxsfHy44DAJVhCyUAAEBFKHAAAAAVocABAABUhAIHAABQEQocAABARZR6CmW9Xt+UZFNRFGXGAAAAqIRSC1xRFAeTHExyT5k5AAAAqsAWSgAAgIpQ4AAAACpCgQMAAKgIBQ4AAKAiFDgAAICKUOAAAAAqQoEDAACoCAUOAACgIhQ4AACAilDgAAAAKkKBAwAAqAgFDgAAoCIUOAAAgIpQ4AAAACpCgQMAAKiI5jJ/83q9vinJpqIoyowBAABQCaUWuKIoDiY5mOSeMnMAAABUgS2UAAAAFaHAAQAAVIQCBwAAUBEKHAAAQEUocAAAABWhwAEAAFSEAgcAAFARChwAAEBFKHAAAAAVocABAABUhAIHAABQEQocAABARShwAAAAFaHAAQAAVIQCBwAAUBEKHAAAQEUocAAAABWhwAEAAFSEAgcAAFARzWX+5vV6fVOSTUVRlBkDAACgEkotcEVRHExyMMk9ZeYAAACoAlsoAQAAKkKBAwAAqAgFDgAAoCIUOAAAgIpQ4AAAACpCgQMAAKgIBQ4AAKAiFDgAAICKUOAAAAAqQoEDAACoCAUOAACgIhQ4AACAilDgAAAAKkKBAwAAqAgFDgAAoCIUOAAAgIpQ4AAAACpCgQMAAKgIBQ4AAKAiFDgAAICKUOAAAAAqQoEDYMk1Go10dnZm9erV6ezsTKPRKDsSAFRCc9kBALi8NBqN7NmzJ4ODg9m4cWMOHTqUnp6eJElXV1fJ6QBgebMCB8CSGhoayuDgYDo6OtLS0pKOjo4MDg5maGio7GgAsOwpcAAsqdHR0bS3t88Ya29vz+joaEmJAKA6FDiAReRerzer1WoZGRmZMTYyMpJarVZSIgCoDgUOYJGcvddr586dOXHiRHbu3Jk9e/Zc9iWuu7s7PT09GR4eztTUVIaHh9PT05Pu7u6yowHAsucQE4BFcr57vfr7+y/rwzrO/rf39/dn8+bNqdVq2b59+2U9JwBwoUpdgavX65vq9frDZWYAWCzu9Tq/b3zjG3nmmWdy5syZPPPMM/nGN75RdiQAqIRSC1xRFAeLori3zAwAi8W9XrPr6+vLvn37smPHjhw7diw7duzIvn370tfXV3Y0AFj23AMHsEjc6zW7AwcOpLe3N9u2bcuaNWuybdu29Pb25sCBA2VHA4Blzz1wAIvEvV6zm5yczNatW2eMbd26NQ8++GBJiQCgOqzAASyirq6uPPnkk3n11Vfz5JNPXvblLUlaW1uzf//+GWP79+9Pa2trSYkAoDoUOACW1JYtW7Jr167s3bs3ExMT2bt3b3bt2pUtW7aUHQ0Alj1bKAFYUgMDA0mS3bt358EHH0xra2vuuuuuc+MAwPkpcAAsuYGBgQwMDKStrS3j4+NlxwGAyrCFEgAAoCIUOAAAgIpQ4AAAACpCgQMAAKgIBQ5gETUajXR2dmb16tXp7OxMo9EoOxIAUGFOoQRYJI1GI3v27Mng4GA2btyYQ4cOpaenJ0k80BsAuChW4AAWydDQUAYHB9PR0ZGWlpZ0dHRkcHAwQ0NDZUcDACpKgQNYJKOjo2lvb58x1t7entHR0ZISAQBVp8ABLJJarZaRkZEZYyMjI6nVaiUlAgCqToEDWCTd3d3p6enJ8PBwpqamMjw8nJ6ennR3d5cdDQCoKIeYACySrq6ufOMb38gnPvGJTE5OprW1NVu2bHGACQBw0azAASySRqORr371q/niF7+YkydP5otf/GK++tWvepQAAHDRFDiAReIUSgBgoSlwAIvEKZQAwEJzDxzAIqnVarnjjjvyv/7X/8r09HSampryD/7BP3AKJQBw0azAASySFStW5G/+5m8yPT2dJJmens7f/M3fZMUKf/X29fVlw4YNWbVqVTZs2JC+vr6yIwFAJfgpAmCRPPXUU0mSq666asbr2fHLVV9fX/bt25cdO3bk2LFj2bFjR/bt26fEAcAFUOAAFtHHP/7xfPvb386pU6fy7W9/Ox//+MfLjlS6AwcOpLe3N9u2bcuaNWuybdu29Pb25sCBA2VHA4BlT4EDYElNTk5m69atM8a2bt2aycnJkhIBQHUocACL6LHHHsuOHTvy0ksvZceOHXnsscfKjlS61tbW7N+/f8bY/v3709raWlIiAKgOBQ5gkdxyyy1JflRO3v3ud58rLWfHL1dbtmzJrl27snfv3kxMTGTv3r3ZtWtXtmzZUnY0AFj2PEYAYJHceOON+drXvjbr+OVsYGAgSbJ79+48+OCDaW1tzV133XVuHAA4PytwAIvkwIEDeeCBBzI2NpZTp05lbGwsDzzwgMM68qMS98wzz+TUqVN55plnlDcAuEAKHMAicVgHALDQFDiAReKwDgBgoSlwAIvEYR3n12g00tnZmdWrV6ezszONRqPsSABQCQ4xAVgkAwMD+W//7b/lwQcfzIMPPpgkef/733/Z3+/VaDTy6U9/+tz7p59++tz7rq6usmIBQCVYgQNYJH19fTl8+HAeeOCBHDt2LA888EAOHz6cvr6+sqOV6o3l7Y//+I9nHQcAZqfAASySAwcOpLe3N9u2bcuaNWuybdu29Pb2OoXyx1pbW/Ov//W/dk8gAMyBAgewSJxC+dZ27NiRY8eOZceOHWVHAYDKUOAAFolTKN/a7/zO7+Saa67J7/zO75QdBQAqwyEmAItky5YtGRgYyB/+4R/mhRdeyLXXXpsXX3wx/+pf/auyoy0Lr7/++oxXAODtWYEDWCQf/vCH09LSkhdeeCFJ8sILL6SlpSUf/vCHS04GAFSVAgewSAYGBnLVVVelKIqcPHkyRVHkqquuuuwfIwAAXDwFDmCRHD16NJ/97GfT0dGRlpaWdHR05LOf/WyOHj1adjQAoKIUOABKsXXr1jz//PNvOqkTADg/BQ5gkaxduza//uu/nuHh4UxNTWV4eDi//uu/nrVr15YdbVnYv39/3v3ud7/ppE4A4PwUOIBF0tfXl9OnT+f+++/PO9/5ztx///05ffp0+vr6yo4GAFSUxwgALJKurq4kydDQUJqamrJmzZrs2LHj3DgAwFwteIGr1+vvT3JfkrYkXy2K4g8X+vcAqIqurq50dXWlra0t4+PjZccBACruggpcvV7/t0k+luT5oig+9Ibx25J8LsnKJH9cFMXuoiieSvKper2+IsnnFyEzAADAZelC74H7QpLb3jhQr9dXJvmDJLcn+UCSX6nX6x/48a/dkeT/TfLVBUsKAABwmbugAlcUxdeT/PCnhtuT/J+iKL5TFMVkkkeT/Isff/5XiqL4p0m2LGRYAC4d1157bZqamnLttdeWHQUAKmM+98CtS/LsG94fSfJP6vX6/53kl5KsSnLofF9cr9fvTXJvkhRFkba2tnlEWTjNzc3LJstyYl7ezJzMzrzM9Nhjj2X37t353//7f+fv//2/nx07duTjH/942bGWhePHj2d6ejrHjx8/N+Z7x5+h8zEvszMvszMvb2ZOZlfFeZlPgWuaZWy6KIr/nOQ/v90XF0XxcJKHz37dcrm530EDszMvb2ZOZmdefqLRaGTPnj0ZHBzMxo0bc+jQofT09OTll192EmWSqampGa9JfO/En6HzMS+zMy+zMy9vZk5mt5zm5YYbbrigz5vPc+COJHnPG96vT/KDeVwP4JIyNDSUwcHBdHR0pKWlJR0dHRkcHMzQ0FDZ0QCAiprPCtxfJanV6/UNScaSbE5y54KkArgEjI6Opr29fcZYe3t7RkdHS0q0PIyNjWXdunWzjgMAb+2CVuDq9fqXkvzXJDfX6/Uj9Xr9k0VRvJ7kM0n+PMlTSYqiKL61eFEBqqVWq+V973tf1q1bl1WrVmXdunV53/vel1qtVna0UjUajaxcuXLG2MqVK9NoNEpKBADV0TQ9PV12hiSZ/sEPlsfuy+W0D3Y5MS9vZk5mZ15+4uwqU0tLSx5//PF85CMfOXe/1+W82vR3/+7fzenTp/OzP/uz+Q//4T/kl3/5l/Pf//t/z8qVK/P973+/7Hil82doduZlduZldublzczJ7JbTvPz4HrjZzhmZYT73wAHwNpqbm3PjjTems7MzN954Y5qb57Nz/dJw+vTpvPe9783Jkydz44035uTJk3nve9+b06dPlx0NAJa9Un+SqNfrm5JsKoqizBgAi6bRaOQf/+N/fO7/8P2P//E/8rGPfazsWKV78cUXc8011yRJJiYmcuzYsZITAUA1lFrgiqI4mORgknvKzAGwWH75l3853/nOd2a8Jzl58mQmJiZy5syZjI2N5cyZM2VHAoBKsIUSYBGdOnUqN954Y/7qr/4qN954Y06dOlV2pGWjqalpxisA8PYUOIBFcvagklOnTuWf/bN/dq68Xc4HmJy1evXqc/e8nT59OqtXry45EQBUg7vpARbR2bK2nE65Wg6am5vznve8J0eOHMn69etz/PjxsiMBQCVYgQNYRI1GI52dnVm9enU6Ozs96yw/2jL58ssv59lnn8309HSeffbZvPzyy7ZSAsAFsAIHsEgajUb27NmTwcHBbNy4MYcOHUpPT0+SpKurq+R05WlqaspszyBV4ADg7VmBA1gkQ0NDGRwcTEdHR1paWtLR0ZHBwcEMDQ2VHa1U5ztx0kmUAPD2FDiARTI6Opr29vYZY+3t7RkdHS0p0fLxjne8I2NjYzl16lTGxsbyjne8o+xIAFAJpRa4er2+qV6vP1xmBoDFUqvVMjIyMmNsZGQktVqtpETLx9TUVIaHh2e8AgBvz4O8ARZJd3d3enp6zt0DNzw8nJ6enmzfvr3saKU7depU7r777kxMTGTNmjWejwcAF8ghJgCL5OxBJf39/dm8eXNqtVq2b99+WR9gkiStra2ZnJzMK6+8kiTnXltbW8uMBQCVoMABLKKurq50dXV5DtwbPPPMM9mwYUMmJyfPjbW2tuaZZ54pMRUAVIMCB8CSO1vWFFsAmBunUAIAAFSEFTgAltwHP/jBHD9+/Nz7q6++Ot/61rdKTAQA1WAFDmARNRqNdHZ2ZvXq1ens7Eyj0Sg7UunOlreWlpY0NTWlpaUlx48fzwc/+MGyowHAsqfAASySRqORPXv2ZOfOnTlx4kR27tyZPXv2XPYl7vjx42lubs6BAwfy8ssv58CBA2lubp6xIgcAzE6BA1gkQ0NDGRwcTEdHR1paWtLR0ZHBwcEMDQ2VHa10n/vc52bMy+c+97myIwFAJZRa4Or1+qZ6vf5wmRkAFsvo6Gja29tnjLW3t2d0dLSkRMvHZz/72bd8DwDMrtQCVxTFwaIo7i0zA8BiqdVqGRkZmTE2MjKSWq1WUqLlYcWKFRkdHc0v/MIv5Pvf/35+4Rd+IaOjo1mxwqYQAHg7/rUEWCTd3d3p6enJ8PBwpqamMjw8nJ6ennR3d5cdrVS/93u/l6amphw+fDi1Wi2HDx9OU1NTfu/3fq/saACw7HmMAMAi6erqSpL09/dn8+bNqdVq2b59+7nxy9XZ//6hoaGMjo6mVqulu7v7sp8XALgQChzAIurq6kpXV1fa2toyPj5edpxlw7wAwMWxhRIAAKAiFDgAAICKUOAAAAAqQoEDYMk1Go10dnZm9erV6ezsTKPRKDsSAFSCQ0wAWFKNRiN79uzJ4OBgNm7cmEOHDqWnpydJnEQJAG/DChwAS2poaCi/+Iu/mP7+/lx55ZXp7+/PL/7iL2ZoaKjsaACw7JW6Alev1zcl2VTXM0QxAAAgAElEQVQURZkxAFhChw8fzsTERB566KFzK3D3339/jhw5UnY0AFj2Sl2BK4riYFEU95aZAYCl1dLSkrvvvjsdHR1paWlJR0dH7r777rS0tJQdDQCWPVsoAVhSU1NT+ZM/+ZMMDw9namoqw8PD+ZM/+ZNMTU2VHQ0Alj0FDoAlddNNN816D9xNN91UdjQAWPYUOACWVHd3dxqNRnbu3JkTJ05k586daTQa6e7uLjsaACx7HiMAwJI6+6iA/v7+bN68ObVaLdu3b/cIAQC4AAocAEuuq6srXV1daWtry/j4eNlxAKAybKEEAACoCAUOAACgIhQ4AACAilDgAAAAKkKBAwAAqAgFDgAAoCIUOAAAgIoo9Tlw9Xp9U5JNRVGUGQMAAKASSi1wRVEcTHIwyT1l5gAAAKgCWygBAAAqQoEDAACoCAUOAACgIhQ4AACAilDgAAAAKkKBAwAAqAgFDgAAoCIUOAAAgIpQ4AAAACpCgQMAAKgIBQ4AAKAiFDiARdRoNNLZ2ZnVq1ens7MzjUaj7EgAQIU1lx0A4FLVaDSyZ8+eDA4OZuPGjTl06FB6enqSJF1dXSWnAwCqyAocwCIZGhrK4OBgOjo60tLSko6OjgwODmZoaKjsaABARZW6Alev1zcl2VQURZkxABbF6Oho2tvbZ4y1t7dndHS0pEQAQNWVWuCKojiY5GCSe8rMAbAYarVaRkZG0tHRcW5sZGQktVqtxFQAQJXZQgmwSLq7u9PT05Ph4eFMTU1leHg4PT096e7uLjsaAFBRDjEBWCRnDyrp7+/P5s2bU6vVsn37dgeYAAAXTYEDWERdXV3p6upKW1tbxsfHy44DAFScLZQAAAAVocABAABUhAIHAABQEQocAABARShwAAAAFaHAAQAAVIQCBwAAUBEKHAAAQEUocAAAABWhwAEAAFSEAgcAAFARChzAImo0Guns7Mzq1avT2dmZRqNRdiQAoMKayw4AcKlqNBrZs2dPBgcHs3Hjxhw6dCg9PT1Jkq6urpLTAQBVZAUOYJEMDQ1lcHAwHR0daWlpSUdHRwYHBzM0NFR2NACgohQ4gEUyOjqa9vb2GWPt7e0ZHR0tKREAUHUKHMAiqdVqGRkZmTE2MjKSWq1WUiIAoOpKLXD1en1TvV5/uMwMAIulu7s7PT09GR4eztTUVIaHh9PT05Pu7u6yowEAFVXqISZFURxMcjDJPWXmAFgMZw8q6e/vz+bNm1Or1bJ9+3YHmAAAF80plACLqKurK11dXWlra8v4+HjZcQCAinMPHAAAQEUocAAAABWhwAEAAFSEAgcAAFARChwAAEBFKHAAAAAVocABAABUhAIHAABQEQoczFGj0UhnZ2dWr16dzs7ONBqNsiMBAHCZaC47AFRJo9HInj17Mjg4mI0bN+bQoUPp6elJknR1dZWcDgCAS50VOJiDoaGhDA4OpqOjIy0tLeno6Mjg4GCGhobKjgYAwGXAChzMwejoaNrb22eMtbe3Z3R0tKRELHe1Wi0TExPn3q9Zs8b3CwBw0azAwRzUarWMjIzMGBsZGUmtVispEcvZ2fK2fv36fPvb38769eszMTHh+wUAuGgKHMxBd3d3enp6Mjw8nKmpqQwPD6enpyfd3d1lR2MZOlve/vIv/zJ/7+/9vfzlX/7luRIHAHAxbKGEOTh7UEl/f382b96cWq2W7du3O8CE8/rkJz+Zzs7OjI6Oplar5ZOf/GT+zb/5N2XHAgAqSoGDOerq6kpXV1fa2toyPj5edhyWuZ07d+bRRx89d2rp5s2by44EAFSYLZQAi6SpqSlnzpzJb/7mb+b73/9+fvM3fzNnzpxJU1NT2dEAgIqyAgewSJqamjI9PZ0jR47kAx/4wIxxAICLYQUOYJGcOXPm3MdvfFbgG8cBAOZCgQNYZM3Nzenu7k5zs00PAMD8KHAAi+z111+f8QoAcLEUOJijRqORzs7OrF69Op2dnWk0GmVHAgDgMqHAwRw0Go3s2bMnO3fuzIkTJ7Jz587s2bNHieNt/e7v/m7ZEQCAS4ACB3MwNDSUwcHBdHR0pKWlJR0dHRkcHJxxQAXM5jd+4zfKjgAAXAIUOJiD0dHRHD16dMYWyqNHj2Z0dLTsaKXr6+vLhg0bsmrVqmzYsCF9fX1lRwIAuOQocDAH1113Xe677748/fTTOXPmTJ5++uncd999ue6668qOVqq+vr7s27cvO3bsyLFjx7Jjx47s27dPiQMAWGAKHMzB0aNH5zR+uThw4EB6e3uzbdu2rFmzJtu2bUtvb28OHDhQdjQAgEtKqQWuXq9vqtfrD5eZAZi/ycnJbN26dcbY1q1bMzk5WVIiAIBLU6kFriiKg0VR3FtmBmD+Wltbs3///hlj+/fvT2tra0mJWO7cMwkAF8cWSmDetmzZkl27dmXv3r2ZmJjI3r17s2vXrmzZsqXsaCxD7pkEgIvXXHYAoPoGBgbyne98Jzt37syDDz6Ypqam/PN//s8zMDBQdjSWodnumUyS3bt3+54BgLdhBQ6Yt0ajkW9+85tZv359mpqasn79+nzzm9/0gHNmNTk5mWuuuWbG4ziuueYa90wCwAVomp6eLjtDkkz/4Ac/KDtDkqStrS3j4+Nlx1h2Lsd5Wbdu3YJcZ2xsbEGus5x9+MMfzvPPP5/Tp0+fG1u5cmXe/e535xvf+EaJycr1Vt9Dl8P3xfm8973vzenTp/PGf3+ampqycuXKfO973ysx2fJwOf59eyHMy+zMy+zMy5uZk9ktp3m54YYbkqTp7T7PFko4j9l+wPYD+exme4zC6dOnL/vHKzC7s+Xtpptuyn/6T/8pH/3oR3P48OEZ/wMAAJidLZTAgrniiivS1NSUK664ouwoLGPT09NZv359vvvd76ZWq+W73/1u1q9fn2WyIwQAljUFDubgfKtsl/Pq2xu99tprmZ6ezmuvvVZ2FJa53/7t384zzzyTU6dO5Zlnnslv//Zvlx0JACrBFkqYo7Nl7fQ9d2Tl579Scprl5ewWOFvheDv33HNP2REAoJKswAGwpJqamt7yFQA4PwUOgCU1PT2dlpaWc/e8/fR7AOD8bKHksnT6vjuTiZPzv849d8w/zJp3ZOXnHpn/dSjNxTxy4nxfc7ncT/lf/st/yXve855zxzc/++yz+bmf+7myYwHAsqfAcXmaODnv+9cW6rkhC1ICKdX5SpfHTpyfsgYAF0eB47J06NZ9yWPH53mV+X79j926L5sW5kosM2NjY7OWuMu9vJ3V1NSUgwcPZtOmTbZPAsAFUuC4LG184q7ltQL3cadZXqqcWnp+09PT+djHPlZ2DACoFIeYALDknnjiiYyNjeXUqVMZGxvLE088UXYkAKgEK3BctuZ779lzC5Qja96xUFeCyvilX/qlPPXUUzPeAwBvT4HjsrQQW9lsiYOLs3Llypw4ceJN9weuXLmypEQAUB22UAKwpM5X1BQ4AHh7ChwAS2pycjJtbW0z7oFra2vL5ORk2dEAYNlT4ABYcv/+3//7t3wPAMzOPXAwRzPu2/nxx57rBXPzL//lv8z//J//c8Z7AODtKXAwB7M9lPnsuBIHF6a1tTXj4+Nv+vPU2tpaUiIAqA5bKAFYUue71809cADw9qzAwXmcb7Vtrp9vZQ5md/bwktlW4wCA2SlwcB6zFa+3+iFTUbs0nb7vzmTi5MJca54Pj8+ad2Tl5x5ZkCxlu/3229PZ2ZnR0dHUarXcfvvt+bM/+7OyYwHAsqfAAbyViZML8sD2sytN8zHvAriM/Nmf/VmKosjGjRtz6NCh1Ov1siMBQCW4Bw6AUtTr9ezZs0d5A4A5UOAAWFIrVvzkn56dO3fOOg4AzM6/lgAsqTNnzpz7+Dd+4zdmHQcAZqfAAVCK5ubm/O7v/m6am92ODQAXSoEDoBSvv/76jFcA4O0pcACU4tprr01TU1OuvfbasqMAQGXYtwJAKV544YUZrwDA27MCBwAAUBEKHAAAQEUocHARbrrppoyOjuamm24qOwpU1tnnvnn+GwBcOPfAwUU4fPhwarVa2TFYAodu3Zc8dnwBrrQA17h1XzbN/yrLxtnnvnn+GwBcOAUOLkJra2smJyfPvXLp2vTxqxfkOqfvuSMrP/+VBbkWAHD5sm8FLsLZ0qa8AQCwlBQ4mIPW1tY5jQMAwEJS4GAOzrfiZiUOAICloMDBRdi6dWuef/75bN26tewoAABcRhxiAhfhS1/6Uvbv35/mZn+EAABYOlbg4CK8/vrrM14BAGApWD4ALti6desW7GvGxsbmGwcA4LKzKAWuXq93Jflokncn+YOiKP6fxfh9gKV1vtL1VsVOUbt8LVTh9z0EAD9xwQWuXq//2yQfS/J8URQfesP4bUk+l2Rlkj8uimJ3URSNJI16vX5NksEkChyXjFWrVuXUqVPnfX85Ghsb84M3b6LwA8DCm8s9cF9IctsbB+r1+sokf5Dk9iQfSPIr9Xr9A2/4lL4f/zpcMk6dOpWmpqYkSVNT02Vf3s4aGxvL2NhYvr/xZ899DLO5+uqrkyQtLS0zXs+OAwDnd8EFriiKryf54U8Ntyf5P0VRfKcoiskkjyb5F/V6valer+9J8mdFUfx/CxcXlofp6ekZr8CF+9a3vpWrr746U1NTSZKpqalcffXV+da3vlVyMgBY/uZ7D9y6JM++4f2RJP8kya8luTXJVfV6/f8qiuKPfvoL6/X6vUnuTZKiKNLW1jbPKAujubl52WRZTszLT6xcuTKnT59+03vz8yPPJeZiFuZlpueee+5Hr7/4T3Pdl/+i5DTLi79vZ2deZmdeZmde3syczK6K8zLfAtc0y9h0URRDSYbe6guLong4ycNnv2Z8fHyeURZGW1tblkuW5cS8/ERzc3O+9KUvZePGjTl06FC2bt2a06dPm583MBezMy+zMy8z+ft2duZlduZldublzczJ7JbTvNxwww0X9HnzfQ7ckSTvecP79Ul+MM9rwrJ26tSpHDx4MBMTEzl48KB74AAAWDLzXYH7qyS1er2+IclYks1J7px3Kljm9u/fn/3795cdAwCAy8wFr8DV6/UvJfmvSW6u1+tH6vX6J4uieD3JZ5L8eZKnkhRFUbgLnUvW2rVrz52Yd1ZLS0vWrl1bUiIAAC4nF7wCVxTFr5xn/FCSQwuWCJax2267Lf/u3/27XHvttXnhhRdy7bXX5sUXX8xtt9329l8MAADzNN974OCy8hd/8Rf5zGc+k3e9611ZsWJF3vWud+Uzn/lM/uIvnKIHAMDim+89cHBZGR0dzZ//+Z9n+/bt504tmpqayu///u+XHQ0AgMtAqQWuXq9vSrKpKIoyY8AFq9VqGRkZSUdHx7mxkZGR1Gq1ElNRtnXr1l3oJ77tp4yNjc0zzdI4fd+dycTJhbnWPXfM7wJr3pGVn3tkQbIAwHJXaoEriuJgkoNJ7ikzB1yo7u7u9PT0ZHBwMBs3bszw8HB6enqyffv2sqNRorcqXevXr8/09PS5901NTTly5MhSxFpcEyez8vNfmfdlFuL5O/MugABQIbZQwhx0dXUlSfr7+7N58+bUarVs37793Di80dnytmbNmjz55JPp7OzMxMRE1q9ff2mUOABgySlwMEddXV3p6upakJUDLm1ny9vo6Gja2toyOjqaWq2WiYmJsqMBABXlFEqYo0ajkc7OzqxevTqdnZ1pNBplR2IZ+4//8T++5XsAgLmwAgdz0Gg08sADD2TNmjVJkomJiTzwwANJYhsls/KMQABgIVmBgzkYGBjIypUr89BDD+XEiRN56KGHsnLlygwMDJQdjWWuqamp7AgAwCXAChzMwdGjR/PII4+ko6MjLS0t6ejoyGc/+9nceeedZUdbEAt1NPyCnAp4iR0N/8aTKAEALpbnwAE/sQBHwy/U4S6XytHwq1atyne+851z83LjjTfm1KlTZceat0O37kseO74AV1qAa9y6L5vmfxUAqATPgYM5WLt2bT71qU/lqquuypEjR7J+/fq89NJLWbt2bdnRWKZ+uqxdCuUtSTZ9/OoFuc7pe+5YkOfJAcDlwj1wMAe33XZbTpw4kWeffTbT09N59tlnc+LECQdV8JbWrVuXd73rXVm3bl3ZUQCAilPgYA6+/OUvp6mpKStW/OiPzooVK9LU1JQvf/nLJSdjORobGzv38SuvvDLrOADAXChwMAfHjx/PlVdemUcffTQnT57Mo48+miuvvDLHjy/EvUBcisbGxjI2NpZTp06d+xgA4GIpcDBHt9xyS/r7+3PllVemv78/t9xyS9mRAAC4TChwMEdf+cpX8sMf/jDT09P54Q9/mK98xQEMAAAsDc+BgzloamrK9PR0XnjhhSQ59+ohzZzPbAeX2EYJAFwsBQ7m4OzDmM8WubOvl8pDmhfm2V4LdD/gJfBsrzeWt0cffTSbN28+N67EAQAXQ4GDObrmmmtmHFpyzTXX5NixYyUmWjgbn7hreT3I++OXxvbUsbGxtLW1ZWxszKMEAIB5KfUeuHq9vqlerz9cZgaYq2PHjp1bcZuenr5kyhuL44/+6I/e8j0AwFyUWuCKojhYFMW9ZWaAi3HFFVfMeIXz+dSnPvWW7wEA5sIplHARXn311Rmv8FbWrVuXL3/5y7ZPAgDzpsDBHDU1NeXMmTNJkjNnzjiBkvN640ElZw8w+elxAIC5cIgJzNH09HSuuOKKvPLKK+de4XzOlrWFOtzlUrF+/fqfnN66bl2amppy5MiRckMBQAVYgYOLMDExMeMVuHAzytuPTU9PZ/369SUlAoDqsAIHc/S+970v3/ve92a8/+53v1teoAV2+p475vX1zy1Qjqx5x0JdiZLM9Z6/6elpDz4HgLehwMEcHT9+PI899lg2btyYQ4cO5d57L52DVOf7DLjkRwVwIa5D9Z2veL1VsVPWAOCt2UIJc3D11VfnpZdeyqc//elceeWV+fSnP52XXnopV199ddnRoHJuuummjI6O5qabbio7CgBUhgIHc7Br1660trbmhRdeyJkzZ/LCCy+ktbU1u3btKjsay1Sj0UhnZ2dWr16dzs7ONBqNsiMtG4cPH06tVsvhw4fLjgIAlaHAwRy1tLSkuflHu4+bm5vT0tJSciKWq0ajkT179mTnzp05ceJEdu7cmT179ihxAMBFU+BgDgYGBrJmzZo88sgjOXnyZB555JGsWbMmAwMDZUdjGRoaGsrg4GA6OjrS0tKSjo6ODA4OZmhoqOxoAEBFlVrg6vX6pnq9/nCZGWAujh49mnq9nv7+/lx55ZXp7+9PvV7P0aNHy47GMjQ6Opr29vYZY+3t7RkdHS0pEQBQdaUWuKIoDhZFcekc4cdl4bHHHpuxJe6xxx4rOxLLVK1Wy8jIyIyxkZGR1Gq1khIBAFVnCyXMQXNzc6ampmaMTU1NnbsnDt6ou7s7PT09GR4eztTUVIaHh9PT05Pu7u6yowEAFeWnTpiD06dPZ2pqKnfeeWdef/31NDc3Z9WqVTl9+nTZ0ViGurq6kiT9/f3ZvHlzarVatm/ffm4cAGCuFDiYg+uvvz6vvPJK1q5dm7GxsaxduzYvvfRSrr/++rKjsUx1dXWlq6srbW1tGR8fLzsOAFBxtlDCHP3Mz/xMHnrooZw4cSIPPfRQfuZnfqbsSAAAXCYUOJiD5557Lr29vTNOoezt7c1zzz1XdjSolBUrVrzlewBgdv7FhDmo1WpZu3Ztnnzyybz66qt58skns3btWqcKwhydOXMmTU1NSZKmpqacOXOm5EQAUA0KHMyBUwVh4Zw9vdUprgBw4fyrCXPgVEFYGCtWrDj3SI6pqamsWLHCKhwAXAAFDubIqYIwfz9d1pQ3ALgwtlACAABUhAIHAABQEaUWuHq9vqlerz9cZgYAlt4VV1yR1tbWJElra2uuuOKKkhMBQDWUeg9cURQHkxxMck+ZOQBYWq+88krGxsbO3Uu6bt26siMBQCXYQglAKdatW5eNGzcqbwAwBwocAEvq7rvvPvfxV7/61VnHAYDZeYwAAEtqYGAgSXLgwIFMTk6mtbU1W7ZsOTcOAJyfFTgAAICKUOAAWFJ9fX3Zt29fduzYkWPHjmXHjh3Zt29f+vr6yo4GAMueAgfAkjpw4EB6e3uzbdu2rFmzJtu2bUtvb28OHDhQdjQA+P/bu//4Sur63uPvsJvEXewK11QuJFBTeqCASu1iqERRI1BF1p5W/LASshcry6qlsbfmsotJHrW7iWxuY7lGaQ1S3Saudj96a8riPorrL9AgRPwBulAIFK/uUdHYhRUDSVhy/5hJNsmeZJM952TON3k9Hw8eZL6ZM/s538yZM++Z78wUPQIcAGBRjY6OqqGhYVpbQ0ODRkdHE6oIAIBwEOAAAIuqrKxMvb2909p6e3snH+wNAABmR4ADACyq+vp6tbe3q7u7W8PDw+ru7lZ7e7vq6+uTLg0AgKLHYwQAAItq4nEB27dv19atW1VWVqYNGzbwGAEAAOaBAAcAWHRtbW1qa2tTRUWFhoaGki4HAIBgEOAAAIuusrLyiLZMJpNAJQAAhIVr4AAAi2pqePv85z+ftR0AAGTHGTgAeXHaaafp0KFD0URlpVasWKEf//jHyRaFopbJZFRRUaFMJkN4AwBgnhINcGa2TtI6d0+yDADztJCd7EOHDs05P8Pllrdbb731iOlrrrkmoWoAAAhHogHO3XdL2i1pY5J1AJif2UIXQQ0Ldc0110xbNwhvAADMD9fAAcibiooK3X///aqoqEi6FASgsrJSu3fvZvgkAAALQIADkDdDQ0M699xzuS085jT1zNvll1+etR0AAGRHgAOQV319fUmXgABkMhllMhmNjIxM/gwAAI6OAAcgr9LpdNIlAAAALFkEOAAAAAAIBAEOAAAAAAJBgAOQN2vWrNF9992nNWvWJF0KAADAkpToc+AALC0HDx7Ueeedl3QZAAAASxZn4ADkDWfgAAAACoszcADyhjNwAAAAhcUZOAAAAAAIBAEOQN4cd9xx0/4PAACA/GIvC0DOSkpKJEnPP//8tP9PtAMAACA/CHAAcjY+Pr6gdgAAABwbAhyAvKiurtaZZ56p4447Tmeeeaaqq6uTLgkAAGDJIcAByIvHH39cNTU1+vnPf66amho9/vjjSZcEAACw5BDgAORFVVWVdu3apZe85CXatWuXqqqqki4JAABgyeE5cADyYv/+/ZM/j46OTpsGZjrnnHP05JNPTk6fcMIJ2rdvX4IVAQAQBs7AAcjZ6tWrF9SO5W0ivJ1xxhkaHBzUGWecoSeffFLnnHNO0qUBAFD0CHAAcjY8PKzy8nKdeuqpOu6443TqqaeqvLxcw8PDSZeGIjQR3r72ta/ptNNO09e+9rXJEAcAAOaWaIAzs3VmdkuSNQDIj+OPP17S4UcHTEwD2fT09Mw5DQAAsks0wLn7bne/NskaAOTH2rVrdc899+jZZ5/VPffco7Vr1yZdEorYhg0b5pwGAADZcRMTAHmxd+9enX322Tp48KDWrFmjp556KumSUKROOOEEPfLII3rDG96gL37xi3rLW96iRx55RCeccELSpQEAUPQIcABydvLJJ2toaGgytD311FMqLS1VRUVFwpWhGO3bt0/nnHOOHnnkEaVSKUnchRIAgPniJiYA8uLEE0+Uu+vpp5+Wu+vEE09MuiQUsX379imTyWhkZESZTIbwBgDAPBHgAOTsiSeeUHNzs1pbW7VmzRq1traqublZTzzxRNKlAQAALCkMoQSQs1QqpZNPPllf/epXVVFRoaGhIfX3908OjwMAAEB+cAYOQM4aGxvV1NSk/v5+jY2Nqb+/X01NTWpsbEy6NAAAgCWFM3AAcpZOpyVJra2tWr9+vVKplDZv3jzZDgAAgPwgwAHIi3Q6rXQ6PTmEEgAAAPnHEEoAAAAACAQBDgAAAAACQYADAAAAgEAQ4ACggPr6+lRXV6dVq1aprq5OfX19SZcEAAACxk1MAKBA+vr61NHRoc7OTl166aXas2ePmpqaJIk7dAIAgGPCGTgAKJCuri51dnaqtrZWpaWlqq2tVWdnp7q6upIuDQAABIoAByAvGCp4pMHBQdXU1Exrq6mp0eDgYEIVAQCA0BHgAORsYqjgtm3bdPDgQW3btk0dHR3LPsSlUikNDAxMaxsYGFAqlUqoIgAAEDoCHICcMVQwu8bGRjU1Nam/v19jY2Pq7+9XU1OTGhsbky4NAAAEipuYAMjZ4OCgfvazn6murk6Dg4NKpVJ673vfu+yHCk7cqKS1tVXr169XKpXS5s2buYEJAAA4ZgQ4ADk76aST1N7ero997GOTd1u87rrrdNJJJyVdWuLS6bTS6bQqKio0NDSUdDkAACBwDKEEAAAAgEAQ4ADk7IknnlBLS4taW1u1Zs0atba2qqWlRU888UTSpQEAACwpBDgAOUulUnr00UentT366KPcbREAACDPCHAAcnbBBRfo5ptv1hVXXKFf/epXuuKKK3TzzTfrggsuSLo0AACAJYUAByBnd999t6677jrt2rVLL37xi7Vr1y5dd911uvvuu5MuDQAAYEnhLpQAcjY4OKg77rhD119//eTdFsfGxvTRj3406dISd9FFF+mhhx6anD7rrLP05S9/OcGKikNlZeURbZlMJoFKAAAIC2fgAOQslUppYGBgWtvAwMCyvwZuIrxdfPHFymQyuvjii/XQQw/poosuSrq0RE2Et5KSEt1+++0qKSmZ1g4AAGZHgAOQs8bGRjU1Nam/v19jY2Pq7+9XU1OTGhsbky4tURPhbceOHaqoqNCOHTsmQ9xyV1JSov379+viiy/W/v37J0McAACYW6JDKM1snaR17p5kGQBylE6nJUmtra1av369UqmUNm/ePNm+nHV2dh4xfe655yZUTfHo7e09Yvqqq65KqBoAAMKRaIBz992SdkvamGQdAHKXTqeVTqcnr4FDpKmpSTt27Jg2DamhoV91VLwAACAASURBVEH79++fNg0AAI6OIZQAUCBnnXWW9u7dq6uvvlpDQ0O6+uqrtXfvXp111llJl5a48fFxVVVVae/evaqqqtL4+HjSJQEAEATuQgkABfLlL39ZF110kfbu3Tt5gw7uQhndbbKyslLj4+O67LLLprUDAIC5EeAAoIAmwhpDS6ebCGv0CwAAC8MQSgAAAAAIBAEOAAAAAAJBgAMAAACAQBDgAAAAACAQBDgAKKArr7xSVVVVKi8vV1VVla688sqkSwIAAAEjwAFAgVx55ZW68847ddVVV+kXv/iFrrrqKt15552EOAAAcMx4jAAAFMhdd92lhoYGbd++XS960Yu0fft2SdKnP/3phCsDAACh4gwcABTI+Pi4brjhhmltN9xwg8bHxxOqCAAAhI4ABwAFUlJSohtvvHFa24033qiSkpKEKgIAAKFjCCUAFMiFF16o3t5e9fb2Tmt/3etel1BFAAAgdJyBAwAAAIBAEOAAoEAmbmKSyWQ0MjKiTCajhoYG3XXXXUmXBgAAAkWAA4AC4SYmAAAg3whwAFAg3MQEAADkGwEOAApk4iYmlZWVKi8vV2VlpXp7e3XhhRcmXVri+vr6VFdXp1WrVqmurk59fX1JlwQAQBC4CyUAFMidd965oPbloq+vTx0dHers7NSll16qPXv2qKmpSZKUTqcTrg4AgOLGGTgAKKDS0tJpNzEpLS1NuqTEdXV1qbOzU7W1tSotLVVtba06OzvV1dWVdGkAABQ9AhwAFNDnPve5OaeXo8HBQdXU1Exrq6mp0eDgYEIVAQAQDgIcABTQ29/+9jmnl6NUKqWBgYFpbQMDA0qlUglVBABAOAhwAFBAY2Nj025iMjY2lnRJiWtsbFRTU5P6+/s1Njam/v5+NTU1qbGxMenSAAAoetzEBACwqCZuVNLa2qr169crlUpp8+bN3MAEAIB5IMABQIFlMhlVVFRoaGhIlZWVSZdTFNLptNLp9GS/AACA+WEIJQAU0ObNm+ecBgAAWAgCHAAUUEdHx5zTAAAAC8EQSgAoMIZNAgCAfOEMHAAAAAAEggAHAAVUUlKiTCajkZERZTIZlZSUJF0SAAAIGAEOAAqot7d3zmkAAICFIMABQAE1NDTMOQ0AALAQBDgAKKDx8XFVVVVp7969qqqq0vj4eNIlAQCAgBHgAKBAMpmMpCjEXXbZZZPhbaIdAABgoQhwAFAgUx8fcOaZZ2ZtBwAAWAieAwcABZbJZFRRUaGhoSHCGwAAyAln4ACggE4//fQ5pwEAABaCAAcABfTYY4/NOQ0AALAQBDgAKLDKykq94hWvYPgkAADIGQEOQF709fWprq5Oq1atUl1dnfr6+pIuKXFT7zb58MMPZ20HAABYCAIcgJz19fWpo6ND27Zt08GDB7Vt2zZ1dHQs+xA39Yzbhz70oaztAAAAC0GAA5Czrq4udXZ2qra2VqWlpaqtrVVnZ6e6urqSLq0oZDIZvf/97+fMGwAAyBkBDkDOBgcHVVNTM62tpqZGg4ODCVVUPD7wgQ/MOQ0AALAQBDgAOUulUhoYGJjWNjAwoFQqlVBFxWPq0Mls0wAAAAtBgAOQs8bGRjU1Nam/v19jY2Pq7+9XU1OTGhsbky6tKFRWVurDH/4w174BAICcrUy6AADhS6fTkqTW1latX79eqVRKmzdvnmxfrjKZzGRomzp0kmvhAADAscp7gDOz35XULOlF7n55vpcPACFZvXq1hoeHp00jGnY7s1+4ZhIAgKOb1xBKM/ukmf3CzH44o/1NZvawmT1qZlskyd3/093fVYhiARQnHiOQ3URIqaqq0oMPPqiqqioNDw8v+2sD6RcAAI7dfK+B2yHpTVMbzGyFpJslvVnS2ZLeYWZn57U6AEHgMQLZTYSUe++9V6effrruvffeybCynNEvAAAcu5Lx8fF5zWhmL5V0u7u/LJ5+taQPuvsfx9M3SJK73xhPf36uIZRmdq2ka+PXrB0dHc3hbeTPypUr9dxzzyVdRtGhX45Enxy2atUqHTx4UKWlpZP9MjY2pjVr1uiZZ55JurzElJeX68EHH9Tpp58+2S+PPfaYzj77bI2MjCRdXmLol7mxbcmOfsmOfsmOfjkSfZJdMfVLWVmZJJUcbb5croGrlPSTKdP7JZ1vZi+W1C7plWZ2w0Sgm8ndb5F0Szw5PjQ0lEMp+VNRUaFiqaWY0C9Hok8OS6VS2rNnj2prayf7pb+/X6lUatn30SWXXKJ77713sl8uueQSSaJf6JdZsW3Jjn7Jjn7Jjn45En2SXTH1yymnnDKv+XIJcNnS4bi7/0rSu3NYLoDATDxGoLOzU5deeunkYwQ2b96cdGmJWr16tfbv36/zzz9fX/rSl3TJJZdo//79y/5GJvQLAADHLpcAt1/SqVOmqyT9NLdyAISIxwhkNzg4qFQqpf379+vss6NLhLnbIv0CAEAucglw35aUMrNqSRlJ6yVdmZeqAAQnnU4rnU4X1VCEYjARSuiX6egXAACOzXwfI/BZSd+SdKaZ7Tezd7n7c5Kuk3SHpIckubvvK1ypAICloq+vT3V1dVq1apXq6uqW/SMnAACYr3mdgXP3d8zSvkfSnrxWBABY0iaeGzhxzeSePXvU1NQkSct+2C0AAEcz3+fAAQCQFzw3EACAY5fLNXA5M7N1kta5e5JlAAAW0eDgoGpqaqa11dTUcBMTAADmIdEA5+67Je2WtDHJOgAAiyeVSmlgYEC1tbWTbQMDA0qlUglWBQBAGBhCCQBYVBPPDezv79fY2NjkcwMbGxuTLg0AgKKX6Bk4AMDyw3MDAQA4dgQ4AMCi47mBAAAcG4ZQAgAAAEAgCHAAgEXX0tKi6upqlZeXq7q6Wi0tLUmXBABAEAhwAIBF1dLSop6eHm3ZskUHDhzQli1b1NPTQ4gDAGAeCHAAgEW1c+dONTc3a9OmTVq9erU2bdqk5uZm7dy5M+nSAAAoeokGODNbZ2a3JFkDAGBxjY6OqqGhYVpbQ0ODRkdHE6oIAIBwJBrg3H23u1+bZA0AgMVVVlam3t7eaW29vb0qKytLqCIAAMLBEEoAwKKqr69Xe3u7uru7NTw8rO7ubrW3t6u+vj7p0gAAKHo8Bw4AsKja2tokSdu3b9fWrVtVVlamDRs2TLYDAIDZEeAAAIuura1NbW1tPMgbAIAFYgglAAAAAASCAAcAAAAAgSDAAUAB9fX1qa6uTqtWrVJdXZ36+vqSLqkotLS0qLq6WuXl5aquruYh3gAAzBMBDgAKpK+vTx0dHdq2bZsOHjyobdu2qaOjY9mHuJaWFvX09GjLli06cOCAtmzZop6eHkIcAADzQIADgALp6upSZ2enamtrVVpaqtraWnV2dqqrqyvp0hK1c+dONTc3a9OmTVq9erU2bdqk5uZm7dy5M+nSAAAoeokGODNbZ2a3JFkDABTK4OCgampqprXV1NRocHAwoYqKw+joqBoaGqa1NTQ0aHR0NKGKAAAIR6IBzt13u/u1SdYAAIWSSqU0MDAwrW1gYECpVCqhiopDWVmZent7p7X19vaqrKwsoYoAAAgHQygBoEAaGxvV1NSk/v5+jY2Nqb+/X01NTWpsbEy6tETV19ervb1d3d3dGh4eVnd3t9rb21VfX590aQAAFD0e5A0ABZJOpyVJra2tWr9+vVKplDZv3jzZvly1tbVJkrZv366tW7eqrKxMGzZsmGwHAACzI8ABQAGl02ml02lVVFRoaGgo6XKKRltbm9ra2ugXAAAWiCGUAAAAABAIAhwAAAAABIIABwAAAACBIMABABZdS0uLqqurVV5erurqarW0tCRdEgAAQSDAAQAWVUtLi3p6erRlyxYdOHBAW7ZsUU9PDyEOAIB5IMABABbVzp071dzcrE2bNmn16tXatGmTmpubtXPnzqRLAwCg6CUa4MxsnZndkmQNAIDFNTo6qoaGhmltDQ0NGh0dTagiAADCkWiAc/fd7n5tkjUAABZXWVmZent7p7X19vaqrKwsoYoAAAgHQygBAIuqvr5e7e3t6u7u1vDwsLq7u9Xe3q76+vqkSwMAoOitTLoAAMDy0tbWJknavn27tm7dqrKyMm3YsGGyHQAAzI4ABwBYdG1tbWpra1NFRYWGhoaSLgcAgGAwhBIAAAAAAkGAAwAAAIBAEOAAAAAAIBAEOAAAAAAIBAEOAAAAAAJBgAMAAACAQBDgAAAAACAQBDgAAAAACESiD/I2s3WS1rl7kmUAAAAAQBASDXDuvlvSbkkbk6wDAAAAAELAEEoAAAAACAQBDgAAAAACQYADAAAAgEAQ4AAAAAAgEAQ4AAAAAAgEAQ4AAAAAAkGAAwAAAIBAEOAAAAAAIBAEOAAAAAAIBAEOAAAAAAJBgAMAAACAQBDgAAAAACAQBDgAAAAACAQBDgAAAAACQYADAAAAgECsTPIfN7N1kta5e5JlAAAAAEAQEg1w7r5b0m5JG5OsAwAAAABCwBBKAAAAAAgEAQ4AAAAAAkGAAwAAAIBAEOAAAAAAIBAEOAAAAAAIBAEOAAAAAAJBgAMAAACAQBDgAAAAACAQBDgAAAAACAQBDgAAAAACQYADAAAAgEAQ4AAAAAAgEAQ4AAAAAAgEAQ4AAAAAAkGAAwAAAIBAEOAAAAAAIBArky4AAABgNpWVlUe0ZTKZBCoBgOLAGTgAAFCUsoW3udoBYDkgwAEAgKKWyWQ0MjLCmTcAUMJDKM1snaR17p5kGQAALJp8nT0izADA8pRogHP33ZJ2S9qYZB0AACyWowWvQxvfqhWfuG2RqgEAhIabmAAAgKLGNW8AcBjXwAEAgKI029lKho8CWM44AwcAAIrWRFirqKjQ0NBQwtUAQPI4AwcAAAAAgSDAAQAAAEAgCHAAAAAAEAgCHAAAAAAEggAHAAAAAIEgwAEAAABAIAhwAAAAABAIAhwAAAAABIIABwAAAACBIMABAAAAQCAIcAAAAAAQCAIcAAAAAASCAAcAAAAAgSDAAQAAAEAgCHAAAAAAEAgCHAAAAAAEggAHAAAAAIEgwAEAAABAIAhwAAAAABAIAhwAAAAABIIABwAAAACBIMABAAAAQCBWJl0AAADAbCorK49oy2QyCVQCAMWBM3AAAKAoZQtvc7UDwHJAgAMAAEUtk8loZGSEM28AoISHUJrZOknr3D3JMpa0Q++7Uhp+es55Ttvznbz8Wz++dO3cM6x+oVZ85DN5+bcAoBjNZ5s7r+VsfGvuxbDNBYAlKdEA5+67Je2WtDHJOpa04ae14hO3zTnLfI5nVlRUaGhoKKdS8rJDAgDFbB7b3KPJx/ZWYpsLAEsVNzEBAABFjWveAOAwroEDAABFabZr3rgWDsByxhk4AABQtCbCWr6GlgJA6DgDBwAAAACBIMABAAAAQCAIcAAAAAAQCAIcAAAAAASCAAcAAAAAgSDAAQAAAEAgCHAAAAAAEAgCHAAAAAAEggAHAAAAAIEgwAEAAABAIAhwAAAAABAIAhwAAAAABIIABwAAAACBIMABAAAAQCAIcAAAAAAQCAIcAAAAAASCAAcAAAAAgSDAAQAAAEAgCHAAAAAAEAgCHAAAAAAEggAHAAAAAIEgwAEAAABAIAhwAAAAABAIAhwAAAAABIIABwAAAACBIMABAAAAQCAIcAAAAAAQCAIcAAAAAASCAAcAAAAAgSDAAQAAAEAgCHAAAAAAEAgCHAAAAAAEggAHAAAAAIEgwAEAAABAIAhwAAAAABAIAhwAAAAABIIABwAAAACBIMABAAAAQCAIcAAAAAAQCAIcAAAAAASCAAcAAAAAgSDAAQAAAEAgVuZ7gWZ2vKR/kDQq6evuvjPf/wYAAAAALEfzOgNnZp80s1+Y2Q9ntL/JzB42s0fNbEvc/GeSPu/uGyW9Nc/1AkBQWlpaVF1drfLyclVXV6ulpSXpkopCX1+f6urqtGrVKtXV1amvry/pkoCgVFZWqrKyUuXl5ZM/g37B8jDfIZQ7JL1paoOZrZB0s6Q3Szpb0jvM7GxJVZJ+Es92KD9lAkB4Wlpa1NPToy1btujAgQPasmWLenp6ln2I6+vrU0dHh7Zt26aDBw9q27Zt6ujoIMQB8zRbKFnuYYV+wXIxrwDn7ndJ+q8ZzTWSHnX3/3T3UUn/IulPJO1XFOLmvXwAWIp27typ5uZmbdq0SatXr9amTZvU3NysnTuX98jyrq4udXZ2qra2VqWlpaqtrVVnZ6e6urqSLg0ISiaT0cjIiDKZTNKlFBX6BUtdyfj4+LxmNLOXSrrd3V8WT18u6U3ufk083SDpfEmbJX1M0rOSvjnbNXBmdq2kayXJ3deOjo7m9k7yZOXKlXruueeSLiNvPnXzo0mXMM07/+L3ki4hb5baupIv9Mth5eXlOnDggFavXj3ZL8PDwzrxxBM1MjKSdHmJWbVqlQ4ePKjS0tLJfhkbG9OaNWv0zDPPJF1eTtjmZke/5Fd5ebkkaWRkZPIzNLVtuVrK/cJnKLtfNPyxxp/+9ay/P23Pd/L2b/340rVz/r7khb+ll/TekdO/UVZWJkklR5svl5uYZFv4uLv/RtI7j/Zid79F0i0TrxsaGsqhlPypqKhQsdSSD+uuOCEvy8lXvyylvl1q60q+0C+HlZWV6aabbtKmTZsm+6W7u1tlZWXLuo9SqZT27Nmj2trayX7p7+9XKpUKvl/ysc3N52eoWPqTfimMoaGhI/plqby3XCzFfuEzlN3407/Wik/cNuvv53MONl/9cmjjW3NezimnnDKv+XIZ4rhf0qlTpqsk/TSH5QHAklJfX6/29nZ1d3dreHhY3d3dam9vV319fdKlJaqxsVFNTU3q7+/X2NiY+vv71dTUpMbGxqRLA4Iy9WYdOIx+wVKXyxm4b0tKmVm1ooC7XtKVeakKAJaAtrY2SdL27du1detWlZWVacOGDZPty1U6nZYktba2av369UqlUtq8efNkO4C5ZTKZrOFkuV/zRb9guZhXgDOzz0p6vaQKM9sv6W/c/Z/M7DpJd0haIemT7r6vYJUCQIDa2trU1tbG0NIZ0um00uk0/QIco4lQwmdoOvoFy8G8Apy7v2OW9j2S9uS1IgAAAABAVtzmHwAAAAACQYADAAAAgEDkchOTnJnZOknr3D3JMgAAAAAgCIkGOHffLWm3pI1J1gEAAAAAIWAIJQAAAAAEggAHAAAAAIEgwAEAAABAIAhwAAAAABAIAhwAAAAABIIABwAAAACB4DlwAAAAABAIngMHAAAAAIFgCCUAAAAABIIABwAAAACBIMABAAAAQCAIcAAAAAAQCAIcAAAAAASCAAcAAAAAgSDAAQAAAEAgCHAAAAAAEIhEH+RtZuskrXP3JMsAAAAAgCAkGuDcfbek3ZI2JlkHAAAAAISAIZQAAAAAEAgCHAAAAAAEggAHAAAAAIEgwAEAAABAIAhwAAAAABAIAhwAAAAABIIABwAAAACBIMABAAAAQCAIcAAAAAAQiJLx8fGka5CkoigCAAAAABJUcrQZiuUMXEmx/Gdm30m6hmL8j36hT+gX+oV+oU/ol+L7j36hX+iTJdcvR1UsAQ4AAAAAcBQEOAAAAAAIBAHuSLckXUCRol+ORJ9kR79kR79kR78ciT7Jjn7Jjn7Jjn45En2SXXD9Uiw3MQEAAAAAHAVn4AAAAAAgEAQ4HBMz22FmlyddR67M7KVm9sMFzP9uM9twlHmuNrOPzfK7Dyy0xlCY2a1mdvZR5sm63sR/hysLVx2QfwvdDi50ezPjtXnddpjZ183svHwuM9+WyvfMYjOz15vZ7UnXsdjM7EdmVpF0HcUmXh8umDJ91P2Y5W7q9tHM9pjZCUnXNBMBDovCzFYmXUM+uPvH3b0nh0Us2QDn7te4+4PH+PKXSlpWAW62HWgzO8/MupKoqRiw0z6rBW87zGxFIQoBEJTXS5oMcHnYj1lSjrZ/6u6XuvuTi1XPfC2JneqFMLM+SadKeoGkj7j7LWb2LkmbJf1U0qCkEXe/zsx+W9LHJZ0Wv/yv3L0/ibpzYWatkuol/UTSkKTvSPqCpJsl/bakYUkb3f0/zGyHpIOSzpP03yVd7+6fN7MSSR+VVCfpcU15ToWZrZX095JeGC//anf/mZl9XdLdkmol3SbpwwV/s8dmhZl9QtEGLiPpTySdouz980FJT7t7p5m9StI/SfqNpG9KerO7vyxe5ilm9u+STpf0BXe/3sy2S1plZt+XtM/d6xfxPc6bmV0v6Vl37zKzmySd6+51ZvZGSe+U1CPpbyWVS3pM0jvd/en4793k7vfN9pmK/4kLzeyvNWX9krRd0llx3/yzu9+0eO+4uLj7fZLuS7oOHCk+at0kaVzSA5IOKcv6HG8v/7ekN8fztrn7rhnLWqFovX+9os/Sze7ebWYnS9olaY2i7+j3SHqLZmw7zOwqSY2SyiTdK+m97n7IzJ5WtD3+Y0nvN7NySZ3xsr4t6T3uPlKYHjp2s3xPTf39jySd5+5D8YGPTnd/vZm9UNF303mK+vpv3f3/LmrxOTKz4yW5pCpJKyRtk9Qh6TOS3iCpVNK1km6U9HuS/s7dPz7P9exVim7Q8DZJTyjqq5crWh8+6O7/VvA3WADZ9uVm/H7aZ9XdG8zsdyR9UtH3+i8VfXf9eHErz59Z9mffJOlDitajIUnvkvRuSYfibcZfSnqjpKclfVHR921NvLyXSrrN3V8x237dYr6/Y5FlG+2SWhRtJ38lqd7dn4j35U5RdPB4KN5n+ZSksyU9JGnVlGX+SIe3PX8t6c/jX93q7v9nEd5WVsvxDNyfu/taRRv7RjOrlNQq6Y8kXSzp96fM+xFJN7n7qxRt/G5d7GJzFX/RvU3SKyX9maL3LUUb9L+M+6JJ0j9MednJkl4j6TJFOxiS9KeSzlS04d+o+GiOmZUq+kK4PF7WJyW1T1nWCe7+Oncv1vAmSSlFO0/nSHpSUX/N1T8TPiXp3e7+akU7clP9gaQrFPXXFWZ2qrtvkfSMu/9BsYa32F2SXhv/fJ6kF8Z/59dI+oGijeFF7v6HioLGX099sZmdotk/U1L29WuLpG/EfRNEeDOzPjP7jpntM7Nr47anzezDZvZdM/tKfBBoLm83swEze8TMXhsvI+jhT3nql4llvdHMvmdmPzCzT8ZhJBFmdo6kZkl17n6upPfFv8q2Pv+Zom3AuZIukvR3cTCb6l2Snoq/X14laaOZVSs6E32Hu0+8/vsztx1mdpai7UttPN8hReFHko6X9EN3P1/R53OHpCvcfWKn/T1565Q8meN7aj5aFfXjy939FZK+WoASC+1Nkn7q7ufGBwH/PW7/Sfz98g1Ff8fLFW1Xt8a/n3M9i4fNfVzSn7j7fypaf78ar3NviOc/vtBvrkBm7su9eOIXc3xWPyapJ15PdkoKfaTDzD44SdInJL0tft9vd/cfKVoHboq3H9+YeLG7PySpzMx+N266QpLPY7+uKM3yd/+mpD9y91dK+hdJ1095yVpFn40rFW0Xh+N1oz3+3czlr1V0EPt8RZ/DjWb2ygK+pTktxwDXaGb3S7pH0ZGLBkl3uvt/ufuYpM9NmfciSR+Lj3reJmmNmf3Wolecm9dI+jd3f8bdfy1pt6KjNRdI+lz83roV7YRM6HP35+PhcCfFbRdK+qy7H3L3n+rwl+SZkl4maW+8rBZFRxEnTDsaWKQed/fvxz9/R9ERmbn6R/F46N9y97vjps/MWOZX3P0pd39W0oOSfqdQxRfAdyStjdf1EUnfUvQF8VpJzyg6QtUf983/0JHvrUazf6ak7OtXiLLtQBwv6btxuL1T0t8cZRkr46OffzWPeUORj36Rmb1AxRU+6iR93t2HJMnd/ytuz7Y+v0aHt5dPKHrPr5qxvEskbYg/R/dKerGig0nflvTO+Ajxy+Pt9kxvVLSD8e349W+UNLETdkjSxBmoMxVt3x6Jp/9Z0ba82GT7npqvixSNlpAkufuBfBe3CH4g6SIz6zCz17r7U3H7bVN+f6+7/9rdfynp2fg7aK717CxFByLXTTnLdImkLfE683VF+wITI4xCM3NfLjXld7N9Vl+tw9/VvYr6L2Qz++BaSXe5++PStPc9F5dk8c9XKNpnO9p+XbHK9nevknSHmf1A0v+SdM6U+W9z92finy+U9On4dQ8oOns302sUjaj6jbs/Lelfdfhg96JbVkMozez1ijb2r3b34XjI18OKNnTZHBfP+8wsvw9BSZa24yQ9GR+5zWbq8Jqpr8/2zIkSRUN6Xj3Lsn5z9BITN/X9HlK0EzZX/0jZ+3WuZQbzWXP3sXjIwDsVDYF9QNHR2tMVDZ/d6+7vmGMRC+mbo81bzBrN7E/jnyd2IJ7X4YMWn1a0gZ/LxO8nDhwsBfnoFyl7+PgLSUkNWSlR9m1gtvV5Put1iaKz/HfM/IWZXaho2GSvmf2dH3m9SomioU83ZFnus+5+aMp8IZhPnc/p8EHnF8x4bdDPQ3L3R+Kj+5dKutHMvhT/amLdel7T17PnFX2nzNVvP1PUT69UNJRd8fxvc/eH81V7EmbZlzuWdSLY9WaWPrhf0XZzIXYpOlj9r5LG3X3QzF6uuffrilW2v/tHJf29u98W99kHp/xu5v7p0daHotqeLrczcC+SdCBe2X9f0SnQ1ZJeZ2YnWnQh49umzP8lSRPX7cjM5tqhL1bflLTOzF4QXyvwFkXXdD1uZm+XJDMrMbNzj7KcuyStN7MV8RCNN8TtD0v6bTN7dbys0vg0dsgO6ij9Ex/l/bWZ/VHctH6eyx6LhycUu7sUDR29S9HwnXdL+r6iI321ZvZ7kmRmq83sjBmvHdDsn6nZ/FpSMGe3Z3x5nivpe5q+AzHhaF8IEztlQYX82eSxX6Qi+7KU9BVJNjFUy8z+2xzz3qVo6PSKeLjohYo+F1PdIek9E9sDMzvDzI636DqdX7j7JxRdY/uH8fxTtx1fkXS5mb1kopb4dTP9i6XtUQAAA4hJREFUh6SXTnxeFY84WcB7XizZvqdm+pEOD2ua63v6xEIVWSgWDTsfdvdPK7pe8Q+P8pIJc61nTyrqxw/Fn0spWuf+0qJr55Tk8K8cZduXm2q2z+rdOvxdXa9ovQtVtj4oV/TdWy1Ne9+zfr+6+2OKvn9adfggW6j7ddn+7i9SdG8DKRoxNJu7FA9DN7OXSXrFLPOk4/2e4xVdWvSNLPMtiuUW4P5d0koze0DRRcL3KPrDfkjREJYvKxruNjF8oVHSeWb2gJk9qGgnNiju/m1FwzDuV3TU+z5F769e0rvi0+/7FN24Yy5fUHQzih9I+kfFOwHuPqpoXH5HvKzva8rdjgI2n/55l6RbzOxbinY2n8oyz0y3SHrAzHbmrdLC+IaiYaPfiofmPKvoGrVfSrpa0mfjz9E9mnGNm7vP9ZmazQOSnjOz+83sf+bzjRTIbDsQxyn6PEjRtUwh7yAci3z2S1GFD3ffp+jaiDvj7cLfzzH7FxSt0/crGm5+vbv/fMY8tyr6bHzXokcLdCsK8a+X9H0z+56ioPKReP7JbUc8XLNF0pfiz+FezRjmHdf8rKIz6Z+LhxA9r+h6mKIyx/fUVH8r6SNm9g1Nv+a4TdKJZvbD+O/yBoXn5ZIG4uFqzYre03zMuZ7F2+51km42s/MV7feUKlqPfhhPhyjbvtykOT6rjYqGJz+gaHvyPoUrWx/8UtEwyn+N3/dEINst6U/N7PsWX2s9wy5JVykaThnsft0sf/cPKtr+fUPRzVhm84+Krvd/QNF1cjMPuMndv6toWP+Aov2bW939e/l8DwtRMj4e7BnkvDGzF3p0F72VijaIn3T3LyRdV75MeX+rFR1BuDZeEZGDiX6Nf94i6WR3D/kLIW+WwWeqXFKfpErFRysVfVHcLukmRUOhnlJ0/dYvZ1nG13X4rp0Vku5z95fGR8ub3P2yQr+PfMtTv+yQdLtHd3N8owK4gyJyx/cUAMwfAU6SmXUqGvbzAkXDMd7n7kumY8zsM4puPPECRddN3JhwSUuCmV0h6QZFO5f/T9FtdrPulC43S/0zNRsze9rdX5h0HcWGfsHR8D0FAPNHgAOAPCGoZEe/AACQPwQ4ACggM7tZ0cPsp/qIu38qiXqKBf0CAMCxIcABAAAAQCCW210oAQAAACBYBDgAAAAACAQBDgAAAAACQYADAAAAgEAQ4AAAAAAgEP8fkGkurjlZ0uMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#are there any outliers in the dataset?\n",
    "\n",
    "#boxplot of all the variables\n",
    "plt.figure(figsize=(15, 15))\n",
    "ax = df.boxplot()\n",
    "ax.set_yscale('log')\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the simple statistical information and the boxplots above, we can easily see that there are some incorrect data points there. For example, the blood pressure has negative readings and the maximum readings for both Systolic blood pressure (ap_hi) and Diastolic blood pressure (ap_lo) are over 10,000, which are obviously mistakes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66169, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to only keep the entries between 97.5% quantile and 2.5% quantile for ap_hi and ap_lo\n",
    "df.drop(df[(df['ap_hi'] > df['ap_hi'].quantile(0.975)) | (df['ap_hi'] < df['ap_hi'].quantile(0.025))].index,inplace=True)\n",
    "df.drop(df[(df['ap_lo'] > df['ap_lo'].quantile(0.975)) | (df['ap_lo'] < df['ap_lo'].quantile(0.025))].index,inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice that there are some abnormal values in column \"weight\" and \"height\". The minimum weight is 10 kg and maximum 200 kg. and the minimum height is 55 cm and maximum is 250 cm. Obviously these values are outliers as they don't represent the values for normal people. So, we decided to remove these by only keeping the values from 2.5 to 97.5 percentile range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[(df['weight'] > df['weight'].quantile(0.975)) | (df['weight'] < df['weight'].quantile(0.025))].index,inplace=True)\n",
    "df.drop(df[(df['height'] > df['height'].quantile(0.975)) | (df['height'] < df['height'].quantile(0.025))].index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above statistics table we realized that age measured in days, for better description we decided to convert age variable to be in years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['years'] = (df['age'] / 365).round().astype('int')\n",
    "if 'age' in df:\n",
    "    del df['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  alco  \\\n",
       "0       2     168    62.0    110     80            1     1      0     0   \n",
       "1       1     156    85.0    140     90            3     1      0     0   \n",
       "2       1     165    64.0    130     70            3     1      0     0   \n",
       "3       2     169    82.0    150    100            1     1      0     0   \n",
       "4       1     156    56.0    100     60            1     1      0     0   \n",
       "\n",
       "   active  cardio  years  \n",
       "0       1       0     50  \n",
       "1       1       1     55  \n",
       "2       0       1     52  \n",
       "3       1       1     48  \n",
       "4       0       0     48  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Body mass index (BMI) is commonly used in medical field. It is a key index for relating weight to height. BMI is a person's weight in kilograms (kg) divided by his or her height in meters squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define BMI\n",
    "df['BMI'] = df['weight']/((df['height']/100)**2)\n",
    "# converting BMI range to categorical as 1 = underweight,2 = normal, 3 = overweight and 4 = obese\n",
    "df['BMI'] = df['BMI'].apply(lambda x: 1 if x<18.5 else(2 if x>=18.5 and x<25 else( 3 if x >= 25 and x < 30 else 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in CDV dataset gender shown as 1 for female and 2 for men. Since this is a binary feature showing these 2 categories as 1 and 2 does not make sense, so we are change the gender levels to be 0 for female and 1 for male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change gender levels: 1 to 0 (female) and 2 to 1 (male)\n",
    "df['gender'] = df['gender'].apply(lambda x:0 if x == 1 else(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "### Data Preparation Part 2<a id=\"Data_Preparation_Part_2\"></a>\n",
    "* Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final dataset includes 13 features and 60728 entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 60728 entries, 0 to 69999\n",
      "Data columns (total 13 columns):\n",
      "gender         60728 non-null int64\n",
      "height         60728 non-null int64\n",
      "weight         60728 non-null float64\n",
      "ap_hi          60728 non-null int64\n",
      "ap_lo          60728 non-null int64\n",
      "cholesterol    60728 non-null int64\n",
      "gluc           60728 non-null int64\n",
      "smoke          60728 non-null int64\n",
      "alco           60728 non-null int64\n",
      "active         60728 non-null int64\n",
      "cardio         60728 non-null int64\n",
      "years          60728 non-null int32\n",
      "BMI            60728 non-null int64\n",
      "dtypes: float64(1), int32(1), int64(11)\n",
      "memory usage: 6.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Feature   |Variable Type   |Variable   |Value Type   |\n",
    "|:---------|:--------------|:---------------|:------------|\n",
    "| Years | Objective Feature | years | int (years) |\n",
    "| Height | Objective Feature | height | int (cm) |\n",
    "| Weight | Objective Feature | weight | float (kg) |\n",
    "| Gender | Objective Feature | gender | binary |\n",
    "| Systolic blood pressure | Examination Feature | ap_hi | int |\n",
    "| Diastolic blood pressure | Examination Feature | ap_lo | int |\n",
    "| Cholesterol | Examination Feature | cholesterol | 1: normal, 2: above normal, 3: well above normal |\n",
    "| Glucose | Examination Feature | gluc | 1: normal, 2: above normal, 3: well above normal |\n",
    "| Smoking | Subjective Feature | smoke | binary |\n",
    "| Alcohol intake | Subjective Feature | alco | binary |\n",
    "| Physical activity | Subjective Feature | active | binary |\n",
    "| Body Mass Index | Examination Feature | bmi | int |\n",
    "| Presence or absence of cardiovascular disease | Target Variable | cardio | binary |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "### Modeling and Evaluation 1<a id=\"Modeling_and_Evaluation_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this LAB was to determine which medical aspects had the most bearing on whether a patient would had CVD or not and also to classified cholesterol level each patient into normal and high cholesterol level.\n",
    "\n",
    "A receiver operating characteristic curve, ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied which is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n",
    "A ROC curve can be used to select a threshold for a classifier which maximizes the true positives, while minimizing the false positives.\n",
    "\n",
    "The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random. Most classifiers have AUCs that fall somewhere between these two values. Therefore, the overall model performances can be compared by considering the AUC.\n",
    "\n",
    "In addition to ROC-AUC metric, we use the other classification metrics in our models too. \n",
    "\n",
    "1- Accuracy: Is the proportion of the total number of predictions that were correct over all kinds predictions made. Accuracy is a good measure when the target variable classes in the data are nearly balanced.\n",
    "\n",
    "Accuracy = (TP + TN)/(TP + FP + FN + TN)\n",
    "\n",
    "2- Precision: Which is also called Positive Predictive Value and is the proportion of positive cases that were correctly identified.\n",
    "\n",
    "Precision = (TP) / (TP + FP)\n",
    "\n",
    "3- Recall or Sensitivity: Is the proportion of actual positive cases which are correctly identified.\n",
    "\n",
    "Recall = (TP) / (TP + FN)\n",
    "\n",
    "Recall gives us information about a model performance with respect to false negatives (how many did we miss), while precision gives us information about its performance with respect to false positives (how many did we caught).\n",
    "\n",
    "So basically, if we want to focus more on minimizing False Negatives, we would want our Recall to be as close to 100% as possible without precision being too bad and if we want to minimize False positives, then our focus should be to make Precision as close to 100% as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "### Modeling and Evaluation 2<a id=\"Modeling_and_Evaluation_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before carrying out the modeling, we have to decide which method we are going to choose for the cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    50.801937\n",
       "1    49.198063\n",
       "Name: cardio, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cardio percentage split\n",
    "(df['cardio'].value_counts()/len(df))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    75.754183\n",
       "2    13.091161\n",
       "3    11.154657\n",
       "Name: cholesterol, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cholesterol percentage split\n",
    "(df['cholesterol'].value_counts()/len(df))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data training, a Stratified k-fold cross validation technique are used which works to balance the ratio of labels used in each fold. Our dependent variable (cardio) has approximately a 49-51% split (see above), and with a large enough data set we can have a high level of confidence of a random split in the data with using Stratified techniques. In this technique dataset will split into 10 equal sections, train on nine of the sections and score against the last section. This method will cycle through 10 times so that the each of the 10 sections is used as a holdout sample. We chose to shuffle the data during the cross validation to provide a higher confidence there was no grouping of schools that we did not notice.\n",
    "\n",
    "In other hand for Cholesterol there is imbalanced distribution between classes (see above). The major class is patients with normal cholesterol level(75.7%) followed by above normal (13%) and well above normal (11%). Since the important thing is that to know if patient has normal cholesterol level or not we decided to combine both above normal classes (2 and 3) and use synthetic minority oversampling technique (SMOTE) which takes a subset of data from the minority class as an example and then new synthetic similar instances are created. These synthetic instances are then added to the original dataset. The new dataset is used as a sample to train the classification models.\n",
    "\n",
    "We chose a K fold (10 folds) validation algorithm, however a Shuffle Split may have performed just as well with this size data set. Where Shuffle Split is capable of creating n folds and fitting the data using n-1 to train and 1 to test against, there is a chance the same data will appear in the test set each time the data is sampled. We chose K fold to insure each value is used in the training set.\n",
    "\n",
    "With the 10-fold cross validation we will be using a grid search technique, which will test a number of different parameters to determine the best final model. Different classification algorithms will have different parameters that can be set, so these will be tested with the grid search method.\n",
    "\n",
    "Since CVD dataset does not have so many features (only 12) dimensionality reduction techniques are not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our basic pipeline includes observation scaling and model classification steps. Observation scaling is a common step for all models. For this step the 'standardscaler' is used, which will scale our variables. This improves our prediction outcomes and makes later feature interpretation significantly easier, since all coefficients will be on the same scale.\n",
    "\n",
    "The same random seed of 101 is used for all models that take a random state seed to eliminate the variability of getting different results between running our models so that we are able to hold which model is going to be the best model, for later interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "## Task 1<a id=\"Task_1\"></a>\n",
    "\n",
    "### 1.1 Modeling and Evaluation 3<a id=\"1.1_Modeling_and_Evaluation_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cardio prediction we tried 4 different algorithms to create a model which has superior prediction capabilities based on the ROC/AUC scoring parameter:\n",
    "\n",
    "* Support Vector Machine (SVM)\n",
    "* K-Nearest Neighbor\n",
    "* Random Forest Classification\n",
    "* Decision Tree Classifier\n",
    "\n",
    "A ROC/AUC plot will be created for each model and summarized our findings based on the combined results for these different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a cross validation using the best parameters for the model which it is passed we use below code that borrowed form  Dr. Jake Drew's 2017HighlySegregatedHighSchoolCampuses Jupyter Notebook provided to the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "def EvaluateClassifierEstimator(classifierEstimator, X, Y, cv):\n",
    "   \n",
    "    #Perform cross validation \n",
    "    scores = cross_validate(classifierEstimator, X, Y, scoring=['accuracy','precision','recall']\n",
    "                            , cv=cv, return_train_score=True)\n",
    "\n",
    "    Accavg = scores['test_accuracy'].mean()\n",
    "    Preavg = scores['test_precision'].mean()\n",
    "    Rreavg = scores['test_recall'].mean()\n",
    "    \n",
    "    print_str = \"The average accuracy for all cv folds is: \\t\\t\\t {Accavg:.5}\"\n",
    "    print_str2 = \"The average precision for all cv folds is: \\t\\t\\t {Preavg:.5}\"\n",
    "    print_str3 = \"The average Recall for all cv folds is: \\t\\t\\t {Rreavg:.5}\"\n",
    "\n",
    "    print(print_str.format(Accavg=Accavg))\n",
    "    print(print_str2.format(Preavg=Preavg))\n",
    "    print(print_str3.format(Rreavg=Rreavg))\n",
    "    print('*********************************************************')\n",
    "\n",
    "    print('Cross Validation Fold Mean Error Scores')\n",
    "    scoresResults = pd.DataFrame()\n",
    "    scoresResults['Accuracy'] = scores['test_accuracy']\n",
    "    scoresResults['Precision'] = scores['test_precision']\n",
    "    scoresResults['Recall'] = scores['test_recall']\n",
    "    print(scoresResults)\n",
    "    return scoresResults\n",
    "\n",
    "def EvaluateClassifierEstimator2(classifierEstimator, X, Y, cv):\n",
    "    \n",
    "    #Perform cross validation \n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    predictions = cross_val_predict(classifierEstimator, X, Y, cv=cv)\n",
    "    \n",
    "    #model evaluation \n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    \n",
    "    #pass true test set values and predictions to classification_report\n",
    "    classReport = classification_report(Y,predictions)\n",
    "    confMat = confusion_matrix(Y,predictions)\n",
    "    acc = accuracy_score(Y,predictions)\n",
    "    \n",
    "    print(classReport)\n",
    "    print(confMat)\n",
    "    print(acc)\n",
    "    \n",
    "def EvaluateClassifierEstimator3(classifierEstimator, X, Y, cv):\n",
    "    from sklearn import metrics as mt\n",
    "    for fold, (train_index, test_index) in enumerate(cv.split(X,Y)):\n",
    "        X_train = X[train_index]    \n",
    "        y_train = Y[train_index]  # Based on your code, you might need a ravel call here, but I would look into how you're generating your y\n",
    "        X_test = X[test_index]\n",
    "        y_test = Y[test_index]  # See comment on ravel and  y_train\n",
    "        sm = SMOTE(random_state=101)\n",
    "        X_train_oversampled, y_train_oversampled = sm.fit_sample(X_train, y_train)\n",
    "        classifierEstimator.fit(X_train, y_train)  \n",
    "        y_pred = classifierEstimator.predict(X_test)\n",
    "        acc = mt.accuracy_score(y_test,y_pred)\n",
    "        conf = mt.confusion_matrix(y_test,y_pred)\n",
    "        print(\"====Iteration\",fold,\" ====\")\n",
    "        print(\"accuracy\", acc )\n",
    "        print(\"confusion matrix\\n\",conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e58e0ed7f212>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#This makes our model's coefficients take on the same scale for accurate feature importance analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics as mt\n",
    "cv = StratifiedKFold(n_splits=10,shuffle=True,random_state=101)\n",
    "#separating input data into two parts X (features) and Y (target)\n",
    "features = [\"gender\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\", \"cholesterol\", \"gluc\", \"smoke\", \"alco\", \"active\", \"years\", \"BMI\"]\n",
    "\n",
    "X2 = df[features].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "#This makes our model's coefficients take on the same scale for accurate feature importance analysis\n",
    "#Notice we scaled the data before the cross validation\n",
    "X = scaler.transform(X2)\n",
    "\n",
    "Y= df[['cardio']].copy()\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 : Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM_SGD 10-fold cross-validation \n",
    "\n",
    "param_grid = { 'loss': ['modified_huber']\n",
    "              ,'penalty':['l2']\n",
    "              ,'alpha': [0.01, 0.1, 1, 10]\n",
    "              ,'class_weight': ['balanced', None]\n",
    "              ,'random_state': [101]\n",
    "              ,'max_iter':[1000,1500]\n",
    "              \n",
    "             }\n",
    "clf_SVM = SGDClassifier()\n",
    "#Create a grid search object using the above parameters \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "SVMGridSearch = GridSearchCV(clf_SVM, param_grid=param_grid, cv=cv,n_jobs=8, verbose=1, scoring='roc_auc' )\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "SVMGridSearch.fit(X,y=Y)\n",
    "y_SVM_score = SVMGridSearch.predict(X)\n",
    "\n",
    "y_SVM_prob=SVMGridSearch.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierEstimaterSVM =SVMGridSearch.best_estimator_\n",
    "classifierEstimaterSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_scores = EvaluateClassifierEstimator(classifierEstimaterSVM,X,Y,cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fprSVM = dict()\n",
    "tprSVM = dict()\n",
    "roc_auc_SVM = dict()\n",
    "for i in np.unique(Y):\n",
    "    fprSVM[i], tprSVM[i], _ = mt.roc_curve(Y, y_SVM_prob[:,i], pos_label=i)\n",
    "    roc_auc_SVM[i] = mt.auc(fprSVM[i], tprSVM[i])\n",
    "plt.figure(figsize=(12,8));    \n",
    "for i in np.unique(Y):\n",
    "    plt.plot(fprSVM[i], tprSVM[i], label= ('class %d (area = %0.2f)' % (i, roc_auc_SVM[i])))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "plt.title('Receiver operating characteristic for SVM classifier')\n",
    "plt.legend(loc=\"lower right\")  \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateClassifierEstimator2(classifierEstimaterSVM, X, Y, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #2 KNN Classification Parameter Optimization with GridSearch\n",
    "\n",
    "K-Nearest Neighbor (KNN) classification is valid option for this dataset since the dataset has been preprocessed and it has no missing values.  Parameter selections are critical to the performance of KNN classifiers; therefore, substantial time and effort was put forth to fully investigate the optimal parameters. \n",
    "\n",
    "##### Parameter Analysis:\n",
    "\n",
    "*Algorithms:*  Algorithm used to compute the nearest neighbors can be ‘auto’matically determine the most appropriate algorithm to use for the given dataset/parameters, so it was left as default in our GridSearch\n",
    "\n",
    "##### GridSearch Parameters:\n",
    "\n",
    "*n_neighbors:* Number of neighbors to use in the analysis. Preliminary analyses were conducted to find a desired range of for number of neighbors. From these analyses, it was determined that the optimal number of neighbors is below 15. Above 15, the accuracy plateaus and start to decrease.\n",
    "\n",
    "*Leaf_size:* The leaf size was adjusted, using: 10, 30, and 100 as the parameters. While there is an over-head penalty with using smaller leaves, accuracy may increase, so we will use it in our Grid Search.\n",
    "\n",
    "*Metric:* How distance is measured between datapoints can be adjusted. The 2 options chosen were ‘minkowski’ and ‘euclidean’.\n",
    "\n",
    "*Weights:* Both uniform and distance were looked at. ‘Uniform’ weight-all neighboring points get equal weight. ‘Distance’ weights points by the inverse of their distance.\n",
    "\n",
    "*Predictor Variables:*  \n",
    "Many of the predictor variable have different scaling, so to ensure all variables were treated equally in the analysis, all predictor variables are scaled to have a mean of 0 and Standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "         'weights': ['uniform','distance'],\n",
    "         'leaf_size': [10,30],\n",
    "         'metric': ['minkowski','euclidean'],\n",
    "         'n_neighbors':[3,5,13,15,17],\n",
    "         \n",
    "    }\n",
    "]\n",
    "clf_KNN = KNeighborsClassifier()\n",
    "grid_search_KNN = GridSearchCV(clf_KNN, param_grid=param_grid,cv=k_fold,n_jobs=-1, verbose=1, scoring='roc_auc' )\n",
    "\n",
    "KNearest_model = grid_search_KNN.fit(X, Y)\n",
    "y_KNN_score = grid_search_KNN.predict(X)\n",
    "\n",
    "y_KNN_prob=grid_search_KNN.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierEstimaterKNN = KNearest_model.best_estimator_\n",
    "classifierEstimaterKNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch algorithm determined the following optimal parameters for K-Neighbors model.\n",
    "\n",
    "* Leaf-Size: 10  \n",
    "* Number of Neighbors: 17\n",
    "\n",
    "* Distance Matric: Minkowski  \n",
    "* Weights: Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNearest_scores = EvaluateClassifierEstimator(classifierEstimaterKNN,X,Y,cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateClassifierEstimator2(classifierEstimaterKNN, X, Y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprKNN = dict()\n",
    "tprKNN = dict()\n",
    "roc_auc_KNN = dict()\n",
    "for i in np.unique(Y):\n",
    "    fprKNN[i], tprKNN[i], _ = mt.roc_curve(Y, y_KNN_prob[:, i], pos_label=i)\n",
    "    roc_auc_KNN[i] = mt.auc(fprKNN[i], tprKNN[i])\n",
    "plt.figure(figsize=(12,8));    \n",
    "for i in np.unique(Y):\n",
    "    plt.plot(fprKNN[i], tprKNN[i], label= ('class %d (area = %0.2f)' % (i, roc_auc_KNN[i])))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "plt.title('Receiver operating characteristic for KNN classifier')\n",
    "plt.legend(loc=\"lower right\")  \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the KNN classification analysis with the optimal algorithm parameters that were determined by the parameter GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3 Random Forest\n",
    "\n",
    "One of the most commonly used classifier techniques is random forest, due to its very low bias and general stability when it comes to classification. One method of optimizing a random forest model is to try different parameters to increase performance. Another method of doing so is by utilizing grid search to let random forest decide which combination of hyperparameters would be best implemented in your model. We chose this route as it saves both time and sanity when comparing so many different parameters.\n",
    "\n",
    "We'll start with a baseline random forest for our starting position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "         'n_estimators': [200, 500], \n",
    "         'max_depth': [5,10,15],\n",
    "         'random_state':[101]\n",
    "     }\n",
    "]\n",
    "\n",
    "clf_RF = RandomForestClassifier()\n",
    "grid_search_RF = GridSearchCV(clf_RF, param_grid=param_grid, cv=k_fold,n_jobs=-1, verbose=1, scoring='roc_auc' )\n",
    "\n",
    "RandomForest_model = grid_search_RF.fit(X, Y)\n",
    "\n",
    "y_RF_score = grid_search_RF.predict(X)\n",
    "y_RF_prob=grid_search_RF.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierEstimaterRF = RandomForest_model.best_estimator_\n",
    "classifierEstimaterRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random_Forest_scores = EvaluateClassifierEstimator(classifierEstimaterRF,X,Y,cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateClassifierEstimator2(classifierEstimaterRF, X, Y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ROC cuve for Random Forest model:\n",
    "fprRF = dict()\n",
    "tprRF = dict()\n",
    "roc_auc_RF = dict()\n",
    "for i in np.unique(Y):\n",
    "    fprRF[i], tprRF[i], _ = mt.roc_curve(Y, y_RF_prob[:, i], pos_label=i)\n",
    "    roc_auc_RF[i] = mt.auc(fprRF[i], tprRF[i])\n",
    "plt.figure(figsize=(12,8));    \n",
    "for i in np.unique(Y):\n",
    "    plt.plot(fprRF[i], tprRF[i], label= ('class %d (area = %0.2f)' % (i, roc_auc_RF[i])))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "plt.title('Receiver operating characteristic for Random Forest classifier')\n",
    "plt.legend(loc=\"lower right\")  \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #4 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "         'max_depth': [5,10],\n",
    "         'random_state':[101] \n",
    "     }\n",
    "]\n",
    "\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "grid_searchDT = GridSearchCV(clf_DT, param_grid=param_grid, cv=k_fold,n_jobs=-1, verbose=1, scoring='roc_auc')\n",
    "\n",
    "\n",
    "# # Here we are training the model, this is \n",
    "# # what takes the most amount of time to run\n",
    "DT_model = grid_searchDT.fit(X, Y)\n",
    "\n",
    "y_DT_score = grid_searchDT.predict(X)\n",
    "y_DT_prob = grid_searchDT.predict_proba(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierEstimaterDT = DT_model.best_estimator_\n",
    "classifierEstimaterDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_scores = EvaluateClassifierEstimator(classifierEstimaterDT,X,Y,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateClassifierEstimator2(classifierEstimaterDT, X, Y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ROC curve for  Decision Tree Classifier:\n",
    "fprDT = dict()\n",
    "tprDT = dict()\n",
    "roc_auc_DT = dict()\n",
    "for i in np.unique(Y):\n",
    "    fprDT[i], tprDT[i], _ = mt.roc_curve(Y, y_DT_prob[:, i], pos_label=i)\n",
    "    roc_auc_DT[i] = mt.auc(fprDT[i], tprDT[i])\n",
    "plt.figure(figsize=(12,8));    \n",
    "for i in np.unique(Y):\n",
    "    plt.plot(fprDT[i], tprDT[i], label= ('class %d (area = %0.2f)' % (i, roc_auc_DT[i])))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "plt.title('Receiver operating characteristic for Decision Tree Classifier')\n",
    "plt.legend(loc=\"lower right\")  \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "### 1.2 Modeling and Evaluation 4<a id=\"1.2_Modeling_and_Evaluation_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the summary of the performance of 4 tested models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model   |AUC   |Accuracy(%)   |\n",
    "|:---------|:--------------|:---------------|\n",
    "| SVM | 0.78 | 72.0 |\n",
    "| KNN | 0.82 | 71.6 |\n",
    "| Random Forest | 0.82 | 72.9 |\n",
    "| Decision Tree | 0.79 | 72.4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC for both KNN and Random Forest classifier is the same.The best model we found was Random Forest Classification, with a  an accuracy of 72.9%, both highest among 4 models tested. On the other hand, the computation time taken by each model was comparable between each other. This leads to the conclusion that the Random Forest Classification pipeline could be used to accurately predict the cardio disease outcome. To make this clearer, we also plotted all 4 ROC curves within one graph (see below). The shape of each ROC curve is quite similar to each other, represented by curves that bow up to the top left of the plot, suggesting all models have good skills at predicting the outcome. With Random Forest having the largest area under the curve, it means that Random Forest model is the most skillful model among 4 tested for the kind of dataset we have.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9));\n",
    "plt.plot(fprSVM[0], tprSVM[0], color='darkorange', lw=1, label='SVM Calasifier (area = %0.3f)' % roc_auc_SVM[0])\n",
    "plt.plot(fprKNN[0], tprKNN[0], color='red', lw=1, label='KNN Classifier (area = %0.3f)' % roc_auc_KNN[0])\n",
    "plt.plot(fprRF[0], tprRF[0], color='black', lw=1, label='RF Calasifier (area = %0.3f)' % roc_auc_RF[0])\n",
    "plt.plot(fprDT[0], tprDT[0], color='yellow', lw=1, label='DT Calasifier (area = %0.3f)' % roc_auc_DT[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for cardio = 0')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9));\n",
    "plt.plot(fprSVM[1], tprSVM[1], color='green', lw=1, label='SVM Calasifier (area = %0.3f)' % roc_auc_SVM[1])\n",
    "plt.plot(fprKNN[1], tprKNN[1], color='blue', lw=1, label='KNN Classifier (area = %0.3f)' % roc_auc_KNN[1])\n",
    "plt.plot(fprRF[1], tprRF[1], color='black', lw=1, label='RF Calasifier (area = %0.3f)' % roc_auc_RF[1])\n",
    "plt.plot(fprDT[1], tprDT[1], color='magenta', lw=1, label='DT Calasifier (area = %0.3f)' % roc_auc_DT[1])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for cardio = 1')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "### 1.3 Modeling and Evaluation 5<a id=\"1.3_Modeling_and_Evaluation_5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models we implemented: \n",
    "\n",
    "1.  **SVM**<br>\n",
    "        Advantage: \n",
    "            -Can handle large number of dimensions. \n",
    "            -Many kernels to choose from. \n",
    "            -Fairly robust against overfitting. \n",
    "        Disadvantage:\n",
    "            -Memory intensive and time consuming.\n",
    "            -Parameterization is hard. \n",
    "        We used stochastic gradient descent(SGD) classifier to implement SVM to dramatically \n",
    "        increase the speed. \n",
    "\n",
    "2. **K-Nearest Neighbor**<br>\n",
    "        Advantage:\n",
    "            -Model does not need to be trained and incremental learning is done when new data is fed in. \n",
    "        Disadvantage:\n",
    "            -Does not handle large number of dimensions.\n",
    "            -Weighing of attributes needs to be done. Additional work.\n",
    "            -memory intensive. \n",
    "            \n",
    "3. **Random Forest** <br>\n",
    "        Advantage: \n",
    "            -Robust to overfitting. \n",
    "            -Performs well with large number of features.\n",
    "        Disadvantage: \n",
    "            -Learning is slow and integration to improve generated models is not possible. \n",
    "            \n",
    "4. **Decision Tree** \n",
    "       Advantages:\n",
    "            -Compared to other algorithms decision trees requires less effort for data preparation during pre-processing.\n",
    "            -A decision tree does not require normalization of data.\n",
    "            -A decision tree does not require scaling of data as well.\n",
    "            -Missing values in the data also does NOT affect the process of building decision tree to any considerable extent.\n",
    "            -A Decision trees model is very intuitive and easy to explain to technical teams as well as stakeholders.\n",
    "      Disadvantage:\n",
    "            -A small change in the data can cause a large change in the structure of the decision tree causing instability.\n",
    "            -For a Decision tree sometimes calculation can go far more complex compared to other algorithms.\n",
    "            -Decision tree often involves higher time to train the model.\n",
    "            -Decision tree training is relatively expensive as complexity and time taken is more.\n",
    "            -Decision Tree algorithm is inadequate for applying regression and predicting continuous values.\n",
    "            \n",
    "            \n",
    "As we have discussed in the above section, the best model we found was Random Forest Classification, with highest AUC and accuracy among 4 models tested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Comparison of Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "acc_SVM = cross_val_score(classifierEstimaterSVM, X, y=Y, cv=cv)\n",
    "acc_KNN = cross_val_score(classifierEstimaterKNN, X, y=Y, cv=cv)\n",
    "acc_RF = cross_val_score(classifierEstimaterRF, X, y=Y, cv=cv)\n",
    "acc_DT = cross_val_score(classifierEstimaterDT, X, y=Y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 2.26 / np.sqrt(10)\n",
    "\n",
    "e_SVM_KNN = (1-acc_SVM)-(1-acc_KNN)\n",
    "e_SVM_RF = (1-acc_SVM)-(1-acc_RF)\n",
    "e_SVM_DT = (1-acc_SVM)-(1-acc_DT)\n",
    "e_KNN_RF = (1-acc_KNN)-(1-acc_RF)\n",
    "e_KNN_DT = (1-acc_KNN)-(1-acc_DT)\n",
    "e_RF_DT = (1-acc_RF)-(1-acc_DT)\n",
    "\n",
    "stdtot_S_K = np.std(e_SVM_KNN)\n",
    "stdtot_S_R = np.std(e_SVM_RF)\n",
    "stdtot_S_D = np.std(e_SVM_DT)\n",
    "stdtot_K_R = np.std(e_KNN_RF)\n",
    "stdtot_K_D = np.std(e_KNN_DT)\n",
    "stdtot_R_D = np.std(e_RF_DT)\n",
    "\n",
    "\n",
    "\n",
    "dbarSK = np.mean(e_SVM_KNN)\n",
    "dbarSR = np.mean(e_SVM_RF)\n",
    "dbarSD = np.mean(e_SVM_DT)\n",
    "dbarKR = np.mean(e_KNN_RF)\n",
    "dbarKD = np.mean(e_KNN_DT)\n",
    "dbarRD = np.mean(e_RF_DT)\n",
    "\n",
    "\n",
    "print ('Range of SVM_KNN confidence interval:[%0.6f,%0.6f]' % (dbarSK-t*stdtot_S_K,dbarSK+t*stdtot_S_K))\n",
    "print ('Range of SVM_RF confidence interval:[%0.6f,%0.6f]' % (dbarSR-t*stdtot_S_R,dbarSR+t*stdtot_S_R))\n",
    "print ('Range of SVM_DT confidence interval:[%0.6f,%0.6f]' % (dbarSD-t*stdtot_S_D,dbarSD+t*stdtot_S_D))\n",
    "print ('Range of KNN_RF confidence interval:[%0.6f,%0.6f]' % (dbarKR-t*stdtot_K_R,dbarKR+t*stdtot_K_R))\n",
    "print ('Range of KNN_DT confidence interval:[%0.6f,%0.6f]' % (dbarKD-t*stdtot_K_D,dbarKD+t*stdtot_K_D))\n",
    "print ('Range of RF_DT confidence interval:[%0.6f,%0.6f]' % (dbarRD-t*stdtot_R_D,dbarRD+t*stdtot_R_D))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to above confidence interval range for each two models comparison we found none of them contain 0 so, we are 95% confident that none of those 4 models are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "### 1.4 Modeling and Evaluation 6<a id=\"1.4_Modeling_and_Evaluation_6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found out Random Forest classifier has the best performance on cardio in CVD dataset. Now, we proceed to find the Level of importance of each feature in this model.\n",
    "\n",
    "Whit this process we are trying to select those features that contribute most to the outcome we are trying to predict.    \n",
    "\n",
    "The benefits of feature selection include reduce overfitting, improving accuracy and minimizing computing time. \n",
    "\n",
    "* All features are scaled in the model.\n",
    "* The coefficient values indicate the level of feature influence on model performance, higher value means stronger influence and importance.\n",
    "* The influence values are sorted and top features with strongest influences are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters of this estimator and fit the model\n",
    "\n",
    "classifierEstimaterRF.fit(X, Y)\n",
    "\n",
    "coef = classifierEstimaterRF.feature_importances_\n",
    "\n",
    "feature_names=list(X2.columns.values)\n",
    "\n",
    "#Creates a new dataframe with the coefficients and the features \n",
    "Top_Features = pd.DataFrame({'feature_names':feature_names, 'weights':coef})\n",
    "print(\"The Top Feature are the following\")\n",
    "display(Top_Features.sort_values(by='weights', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.barplot(x =Top_Features['weights'], y = Top_Features.sort_values(by='weights', ascending=False)['feature_names'], \n",
    "                 orient= 'h')\n",
    "ax.set_title(\"Top Feature Correlations\")\n",
    "ax.set_xlabel(\"Coefficient Magnitude\\n(z-score)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the coefficient shown above, it appears that ap_hi is the most important feature, followed by ap_lo, years, cholesterol, weight, height and others. \n",
    "\n",
    "It is obvious that Systolic blood pressure (ap_hi) holds the most weight in our prediction, followed by Diastolic blood pressure (ap_lo). These make a lot of sense as people with high blood pressure will have a bigger burden on their hearts, which would lead to cardio diseases later on. The age of a patient was the third in the prediction of cardo. This would also make sense as the older a person is the more risky of developing cardiovascular problems. We found that the data showed a person in their mid-50’s and older was more at risk. Numerous scientific reports have suggested a positive correlation between cholecsterol level, overweight and cardio diseases. Obviously, our above data supported that. On the other hand, gender, active status and glucose situation appeared to not play significant role in terms of correlating with cardio status. This is understandable. For example, glucose situation should be more related to diabetes, not cardio-disease. Since cardio disease is not caused by genetics in general, the feature of gender should not play a significant role here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most influential features by Recursive Feature Elimination method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "rfecv = RFECV(estimator=classifierEstimaterRF, step=1, cv=None, scoring='roc_auc')\n",
    "rfecv.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine categorical variables of interest  \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find which features were eliminated\n",
    "eliminated_fea1 =[]\n",
    "for i in range(len(rfecv.ranking_)):\n",
    "    if rfecv.support_[i] == False:\n",
    "        eliminated_fea1.append(features[i])\n",
    "    \n",
    "    i+1\n",
    "    \n",
    "print(\"Eliminated features:\", eliminated_fea1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the RFE Rankings\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "rfe_ft_imp_df = pd.DataFrame({'feature_names':X2.columns.values, 'weights':rfecv.grid_scores_})\n",
    "rfe_ft_imp_df.sort_values(by='weights', inplace=True, ascending=False )\n",
    "\n",
    "features = rfe_ft_imp_df\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from the Recursive Feature Elimination method suggest that all features are important and none should be deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "# Task 2<a id=\"Task_1\"></a>\n",
    "\n",
    "### 2.1 Modeling and Evaluation 3<a id=\"2.1_Modeling_and_Evaluation_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are using 3 different classifiers to predict if patient cholesterol is in normal level or above. As we mentioned previously there is imbalanced distribution in cholesterol classes and we can not use regular cross validation methods. We decided to combine 2 above normal classes together (as not normal cholesterol level, '0') and use SMOTE technique to generate new samples for minor class in order to solve imbalanced situation. This technique is followed to avoid overfitting which occurs when exact replicas of minority instances are added to the main dataset. In addition to SMOTE, there is another technique to deal with imbalanced distribution which called NearMiss. NearMiss is an under-sampling technique. Instead of resampling the Minority class, using a distance, this will make the majority class equal to minority class. For this task we used accuracy as a metric to evaluate 3 different models which are KNN , Random Forest and Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining 2 above normal classes\n",
    "#change gender levels: normal = 0  and not-normal = 1\n",
    "df['cholesterol'] = df['cholesterol'].apply(lambda x:1 if x == 2 or x == 3 else(0))\n",
    "#cholesterol percentage split\n",
    "(df['cholesterol'].value_counts()/len(df))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating input data into two parts X (features) and Y (target)\n",
    "features1 = [\"gender\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\",\"cardio\", \"gluc\", \"smoke\", \"alco\", \"active\", \"years\", \"BMI\"]\n",
    "\n",
    "X1 = df[features1].copy()\n",
    "\n",
    "Y1= df[['cholesterol']].copy()\n",
    "Y2 = Y1.values\n",
    "\n",
    "scaler.fit(X1)\n",
    "\n",
    "#This makes our model's coefficients take on the same scale for accurate feature importance analysis\n",
    "#Notice we scaled the data before the cross validation\n",
    "X_Scl = scaler.transform(X1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test split before resampling\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_Scl, Y1, test_size = 0.2, random_state = 101) \n",
    "\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y1_train['cholesterol'] == 1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y1_train['cholesterol'] == 0))) \n",
    "  \n",
    "# import SMOTE module from imblearn library \n",
    "# pip install imblearn (if you don't have imblearn in your system) \n",
    "from imblearn.over_sampling import SMOTE \n",
    "sm = SMOTE(random_state = 2) \n",
    "X1_train_res, y1_train_res = sm.fit_sample(X1_train, y1_train) \n",
    "  \n",
    "print('After OverSampling, the shape of train_X: {}'.format(X1_train_res.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y1_train_res.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y1_train_res == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y1_train_res == 0))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: KNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = [\n",
    "    {\n",
    "         'weights': ['uniform','distance'],\n",
    "         'leaf_size': [5,10],\n",
    "         'metric': ['minkowski','euclidean'],\n",
    "         'n_neighbors':[2,3,5],\n",
    "         \n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "grid_search_KNN = GridSearchCV(clf_KNN, param_grid=param_grid,cv=cv,n_jobs=-1, verbose=1, scoring='accuracy' )\n",
    "\n",
    "KNearest_model1 = grid_search_KNN.fit(X1_train_res, y1_train_res)\n",
    "\n",
    "y_KNN_score1 = grid_search_KNN.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierEstimaterKNN1 = KNearest_model1.best_estimator_\n",
    "classifierEstimaterKNN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNearest_scores1 = EvaluateClassifierEstimator(classifierEstimaterKNN1, X1_train_res, y1_train_res,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateClassifierEstimator3(classifierEstimaterKNN1, X_Scl, Y2, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "KNN_accuracy = cross_val_score(classifierEstimaterKNN1, X_Scl, y=Y2, cv=cv)\n",
    "KNN_acc=KNN_accuracy.mean()\n",
    "KNN_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #2 Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "         'n_estimators': [200, 500], \n",
    "         'max_depth': [20,30,35],\n",
    "         'random_state':[101]\n",
    "     }\n",
    "]\n",
    "\n",
    "grid_search_RF = GridSearchCV(clf_RF, param_grid=param_grid, cv=k_fold,n_jobs=-1, verbose=1, scoring='accuracy' )\n",
    "\n",
    "RandomForest_model1 = grid_search_RF.fit(X1_train_res, y1_train_res)\n",
    "\n",
    "y_RF_score1 = grid_search_RF.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierEstimaterRF1 = RandomForest_model1.best_estimator_\n",
    "classifierEstimaterRF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateClassifierEstimator3(classifierEstimaterRF1, X_Scl, Y2, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_accuracy = cross_val_score(classifierEstimaterRF1, X_Scl, y=Y2, cv=cv)\n",
    "RF_acc=RF_accuracy.mean()\n",
    "RF_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "parameters = { 'penalty':['l2']\n",
    "              ,'C': [0.1, 1, 10, 100]\n",
    "              ,'class_weight': ['balanced','none']\n",
    "              ,'solver': ['lbfgs']\n",
    "              ,'max_iter':[1500,2000]\n",
    "              ,'random_state':[101]\n",
    "             }\n",
    "\n",
    "clf_LR = LogisticRegression()\n",
    "grid_search_LR = GridSearchCV(clf_LR, param_grid=parameters, cv=k_fold,n_jobs=-1, verbose=1, scoring='accuracy' )\n",
    "\n",
    "LogisticRegression_model = grid_search_LR.fit(X1_train_res, y1_train_res)\n",
    "\n",
    "y_LR_score = grid_search_LR.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierEstimaterLR = LogisticRegression_model.best_estimator_\n",
    "classifierEstimaterLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateClassifierEstimator3(classifierEstimaterLR, X_Scl, Y2, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_accuracy = cross_val_score(classifierEstimaterLR, X_Scl, y=Y2, cv=cv)\n",
    "LR_acc=LR_accuracy.mean()\n",
    "LR_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "### 2.2 Modeling and Evaluation 4<a id=\"2.2_Modeling_and_Evaluation_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model we found was Random Forest Classification, with a accuracy score of ~.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('accuracy for KNN classifier is :',KNN_acc)\n",
    "print ('accuracy for Random Forest classifier is :',RF_acc)\n",
    "print ('accuracy for Logistic Regression classifier is :',LR_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "### 2.3 Modeling and Evaluation 5<a id=\"2.3_Modeling_and_Evaluation_5\"></a>\n",
    "* Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Comparison of Classifiers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 2.26 / np.sqrt(10)\n",
    "\n",
    "e_KNN_LR = (1-KNN_accuracy)-(1-LR_accuracy)\n",
    "e_KNN_RF = (1-KNN_accuracy)-(1-RF_accuracy)\n",
    "e_RF_LR = (1-RF_accuracy)-(1-LR_accuracy)\n",
    "\n",
    "\n",
    "stdtot_K_L = np.std(e_KNN_LR)\n",
    "stdtot_K_R = np.std(e_KNN_RF)\n",
    "stdtot_R_L = np.std(e_RF_LR)\n",
    "\n",
    "\n",
    "\n",
    "dbarKL = np.mean(e_KNN_LR)\n",
    "dbarKR = np.mean(e_KNN_RF)\n",
    "dbarRL = np.mean(e_RF_LR)\n",
    "\n",
    "\n",
    "\n",
    "print ('Range of KNN_LR confidence interval:[%0.6f,%0.6f]' % (dbarKL-t*stdtot_K_L,dbarKL+t*stdtot_K_L))\n",
    "print ('Range of KNN_RF confidence interval:[%0.6f,%0.6f]' % (dbarKR-t*stdtot_K_R,dbarKR+t*stdtot_K_R))\n",
    "print ('Range of RF_LR confidence interval:[%0.6f,%0.6f]' % (dbarRL-t*stdtot_R_L,dbarRL+t*stdtot_R_L))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of above confidence intervals contain 0 so we are 95% confident that those three models are not the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "### 2.4 Modeling and Evaluation 6<a id=\"2.4_Modeling_and_Evaluation_6\"></a>\n",
    "* Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found out Random Forest classifier has the best performance on cholesterol in CVD dataset. Now, we proceed to find the Level of importance of each feature in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters of this estimator and fit the model\n",
    "classifierEstimaterRF1.fit(X_Scl, Y1)\n",
    "\n",
    "coef = classifierEstimaterRF1.feature_importances_\n",
    "\n",
    "feature_names=list(X1.columns.values)\n",
    "\n",
    "#Creates a new dataframe with the coefficients and the features \n",
    "Top_Features = pd.DataFrame({'feature_names':feature_names, 'weights':coef})\n",
    "print(\"The Top Feature are the following\")\n",
    "display(Top_Features.sort_values(by='weights', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.barplot(x =Top_Features['weights'], y = Top_Features.sort_values(by='weights', ascending=False)['feature_names'], \n",
    "                 orient= 'h')\n",
    "ax.set_title(\"Top Feature Correlations\")\n",
    "ax.set_xlabel(\"Coefficient Magnitude\\n(z-score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv1 = RFECV(estimator=classifierEstimaterRF1, step=1, cv=None, scoring='accuracy')\n",
    "rfecv1.fit(X_Scl, Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine categorical variables of interest  \n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv1.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score\")\n",
    "plt.plot(range(1, len(rfecv1.grid_scores_) + 1), rfecv1.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "### Deployment<a id=\"Deployment\"></a>\n",
    "* How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would you deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "### Exceptional Work<a id=\"Exceptional_Work\"></a>\n",
    "* You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be looking at a Voting Classifier. Voting is one of the easyest ways of combining the predictions from multiple machine learning algorithms. Voting classifier isn’t an actual classifier but it wraps up different ones that are trained and valuated in parallel in order to look at the different uniqueness of each system. We will use and ensemble of different algorithms then predict the final output.\n",
    "\n",
    "This output on a prediction is taken by majority vote according to two different strategies:\n",
    "\n",
    "* Hard voting / Majority voting: Hard voting is the simplest case of majority voting. In this case, the class that received the highest number of votes will be chosen.\n",
    "Or\n",
    "* Soft Voting / Average probability: The probability vector for each predicted class are summed and the average is collected. The winning class is the one with the highest value.\n",
    "\n",
    "We will use both hard and soft voting ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "\n",
    "# Training classifiers\n",
    "\n",
    "clf1 = classifierEstimaterSVM\n",
    "\n",
    "clf2 = classifierEstimaterKNN\n",
    "\n",
    "clf3 = classifierEstimaterRF\n",
    "\n",
    "clf4 = classifierEstimaterDT\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('SVM', clf1), ('KNN', clf2),\n",
    "                                    ('RF', clf3),('DT',clf4)],voting='hard',)\n",
    "\n",
    "labels = ['SVM', 'KNN (k=13)', 'Random Forest (depth=10)','Decision Tree (depth=5)','Ensemble']\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3,clf4,eclf], labels):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, Y, \n",
    "                                              cv=StratifiedKFold(n_splits=10, random_state=101), \n",
    "                                              scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1_1 = SGDClassifier(alpha=0.01, average=False,\n",
    "                           class_weight='balanced', early_stopping=False,\n",
    "                           epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
    "                           l1_ratio=0.15, learning_rate='optimal',\n",
    "                           loss='modified_huber', max_iter=1000, n_iter_no_change=5,\n",
    "                           n_jobs=None, penalty='l2')\n",
    "\n",
    "eclf_soft = VotingClassifier(estimators=[('SVM', clf1_1), ('KNN', clf2),\n",
    "                                    ('RF', clf3),('DT',clf4)],voting='soft',)\n",
    "\n",
    "labels = ['SVM', 'KNN (k=13)', 'Random Forest (depth=10)','Decision Tree (depth=5)','Ensemble']\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3,clf4,eclf], labels):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, Y, \n",
    "                                              cv=StratifiedKFold(n_splits=10, random_state=101), \n",
    "                                              scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
