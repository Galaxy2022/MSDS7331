{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 7331 Data Mining\n",
    "### Logistic Regression and SVM\n",
    "### Mini Lab\n",
    "* Allen Ansari<br>\n",
    "* Chad Madding<br>\n",
    "* Yongjun (Ian) Chu<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Cardiovascular diseases (CVD) are the no. 1 cause of death in US each year. To reduce the death rate, the best approach is by early detection and screening. In this Mini Lab we will implemented Logistic Regression (Logit) and Support Vector Machine (SVM) to look at predicting the probablity of a patient having CVD based on results from medical examinations, such as blood pressure values and glucose content. The following categories are used for the analysis:\n",
    "\n",
    "**1) Model Creation**\n",
    "- Create a logistic regression model and a support vector machine model for the classification task involved with our dataset. \n",
    "- Assess how well each model performs (use 80/20 training/testing split for your data).\n",
    "- Adjust parameters of the models to make them more accurate. The SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe. \n",
    "\n",
    "**2) Model Advantages**\n",
    "- Discuss the advantages of each model for each classification task. \n",
    "- Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail.\n",
    "\n",
    "**3) Interpret Feature Importance**\n",
    "- Use the weights from logistic regression to interpret the importance of different features for the classification task.\n",
    "- Explain your interpretation in detail. Why do you think some variables are more important?\n",
    "\n",
    "**4) Interpet Support Vectors**\n",
    "- Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC modelâ€” then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "### Choosing the cadiovascular diseases dataset\n",
    "Cardiovascular diseases (CVD) are the no. 1 cause of death in US each year. To reduce the death rate, the best approach is by early detection and screening. An efficeint way would be to predict the probablity of a patient having CVD based on results from medical examinations, such as blood pressure values and glucose content. \n",
    "\n",
    "Here, we obtained a CVD dataset from Kaggle. It consists of 70,000 records of patients data in 12 features, such as age, gender, systolic blood pressure, diastolic blood pressure and CVD status(binary, 1 or 0). The purpose of this dataset was to determine which medical aspects had the most bearing on whether a patient would had CVD or not. \n",
    "\n",
    "To mine useful knowledge from the dataset, we will establish a prediction algorithm chosen from some commonly used classification models, including logistic regression, to find a relationship between a specific attribute or group of attributes and the probability of having CVD for a patient. To measure the effectiveness of our prediction algorithm, we will use the cross-validation. For each evaluation, accuracy metric for binary classification models called Area Under the (Receiver Operating Characteristic) Curve (AUC) will be generated. AUC measures the ability of the model to predict a higher score for positive examples as compared to negative examples. We can get the overall performance measure by computing the average of the AUC metrics from cross-validations for any particular model. Results from different models will be compared and the best one(s) will be chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data description\n",
    "\n",
    "We will be peforming an analysis of a cleaned up cadiovascular diseases dataset we used from the Lab 1 assigement.\n",
    "\n",
    "Our task is to predict the presence or absence of cardiovascular disease (CVD) using the patient examination results. \n",
    "\n",
    "There are 3 types of input features:\n",
    "\n",
    "- *Objective*: factual information;\n",
    "- *Examination*: results of medical examination;\n",
    "- *Subjective*: information given by the patient.\n",
    "\n",
    "|Feature   |Variable Type   |Variable   |Value Type   |\n",
    "|:---------|:--------------|:---------------|:------------|\n",
    "| Gender | Objective Feature | gender | categorical code |\n",
    "| Height | Objective Feature | height | int (cm) |\n",
    "| Weight | Objective Feature | weight | float (kg) |\n",
    "| Systolic blood pressure | Examination Feature | ap_hi | int |\n",
    "| Diastolic blood pressure | Examination Feature | ap_lo | int |\n",
    "| Cholesterol | Examination Feature | cholesterol | 1: normal, 2: above normal, 3: well above normal |\n",
    "| Glucose | Examination Feature | gluc | 1: normal, 2: above normal, 3: well above normal |\n",
    "| Smoking | Subjective Feature | smoke | binary |\n",
    "| Alcohol intake | Subjective Feature | alco | binary |\n",
    "| Physical activity | Subjective Feature | active | binary |\n",
    "| Presence or absence of cardiovascular disease | Target Variable | cardio | binary |\n",
    "| Years | Objective Feature | years | age in years |\n",
    "| BMI | Objective Feature | bmi | bmi |\n",
    "\n",
    "For any binary data type, \"0\" means \"No\" and \"1\" means \"Yes\". All of the dataset values were collected at the moment of medical examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0   0  18393       2     168    62.0    110     80            1     1      0   \n",
       "1   1  20228       1     156    85.0    140     90            3     1      0   \n",
       "2   2  18857       1     165    64.0    130     70            3     1      0   \n",
       "3   3  17623       2     169    82.0    150    100            1     1      0   \n",
       "4   4  17474       1     156    56.0    100     60            1     1      0   \n",
       "\n",
       "   alco  active  cardio  \n",
       "0     0       1       0  \n",
       "1     0       1       1  \n",
       "2     0       0       1  \n",
       "3     0       1       1  \n",
       "4     0       0       0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "#Bring in data set\n",
    "df = pd.read_csv('Data/cardio_train.csv',sep=';') #read in the csv file\n",
    "\n",
    "# Show the dimention and the first 5 rows of the dataset\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Body mass index (BMI) is commonly used in medical field. It is a key index for relating weight to height. BMI is a person's weight in kilograms (kg) divided by his or her height in meters squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new vaiable as BMI\n",
    "\n",
    "df['BMI'] = df['weight']/((df['height']/100)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66193, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to only keep the entries between 97.5% quantile and 2.5% quantile for ap_hi and ap_lo\n",
    "df.drop(df[(df['ap_hi'] > df['ap_hi'].quantile(0.975)) | (df['ap_hi'] < df['ap_hi'].quantile(0.025))].index,inplace=True)\n",
    "df.drop(df[(df['ap_lo'] > df['ap_lo'].quantile(0.975)) | (df['ap_lo'] < df['ap_lo'].quantile(0.025))].index,inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[(df['weight'] > df['weight'].quantile(0.975)) | (df['weight'] < df['weight'].quantile(0.025))].index,inplace=True)\n",
    "#we want to check how the plot looks like when converting age from days to years\n",
    "df['years'] = (df['age'] / 365).round().astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nominal Data\n",
    "Nominal data usually has more than two values. For logistic regression and SVMs, we created dummy variables that only factor in 0s and 1s for the prediction process of logistic regression and SVMs. In CVD dataset gender is nominal but it is 1 for female and 2 for male which need to be converted to 0 and 1. Also we converted BMI to nominal form for better explanation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender = df.gender.apply(lambda x: 0 if x == 1 else 1)\n",
    "df['BMI'] = df['BMI'].apply(lambda x: 1 if x<18.5 else( 2 if x>=18.5 and x<25 else( 3 if x >= 25 and x < 30 else 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>BMI</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18857</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>17623</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>17474</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0   0  18393       1     168    62.0    110     80            1     1      0   \n",
       "1   1  20228       0     156    85.0    140     90            3     1      0   \n",
       "2   2  18857       0     165    64.0    130     70            3     1      0   \n",
       "3   3  17623       1     169    82.0    150    100            1     1      0   \n",
       "4   4  17474       0     156    56.0    100     60            1     1      0   \n",
       "\n",
       "   alco  active  cardio  BMI  years  \n",
       "0     0       1       0    2     50  \n",
       "1     0       1       1    4     55  \n",
       "2     0       0       1    2     52  \n",
       "3     0       1       1    3     48  \n",
       "4     0       0       0    2     48  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows except first occurrence based on all columns are :\n",
      "Empty DataFrame\n",
      "Columns: [id, age, gender, height, weight, ap_hi, ap_lo, cholesterol, gluc, smoke, alco, active, cardio, BMI, years]\n",
      "Index: []\n",
      "\n",
      "There are 0 duplicated entries in the dataset!\n"
     ]
    }
   ],
   "source": [
    "#Are there any duplicate entries in the dataset?\n",
    "#duplicateRowsDF = df[df.duplicated(keep=False)]\n",
    "duplicateRowsDF = df[df.duplicated(keep='first')]\n",
    "\n",
    "print(\"Duplicate Rows except first occurrence based on all columns are :\")\n",
    "print(duplicateRowsDF)\n",
    "\n",
    "print(f\"\\nThere are {len(duplicateRowsDF)} duplicated entries in the dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 63079 entries, 0 to 69999\n",
      "Data columns (total 12 columns):\n",
      "gender         63079 non-null int64\n",
      "height         63079 non-null int64\n",
      "weight         63079 non-null float64\n",
      "ap_hi          63079 non-null int64\n",
      "ap_lo          63079 non-null int64\n",
      "cholesterol    63079 non-null int64\n",
      "gluc           63079 non-null int64\n",
      "smoke          63079 non-null int64\n",
      "alco           63079 non-null int64\n",
      "active         63079 non-null int64\n",
      "BMI            63079 non-null int64\n",
      "years          63079 non-null int32\n",
      "dtypes: float64(1), int32(1), int64(10)\n",
      "memory usage: 6.0 MB\n"
     ]
    }
   ],
   "source": [
    "del df['age']\n",
    "del df['id']\n",
    "y=df['cardio'].astype(np.int32)\n",
    "del df['cardio']\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Models\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "### SGDClassifier Over the Other Sklearn Functions\n",
    "\n",
    "First, we used SVC setting kernel = 'linear' but waited a long time for it to finish. Then we used LogisticRegression and checked accuracy and precision.\n",
    "\n",
    "At the end we tried SGDClassifier with loss = 'log' which was exponentially faster than the others so this is what we use for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Test Accuracy and scores (MAE, RSME and MAPE)\n",
    "here are the functions that we used to individually find, visualize, and report the best parameters per model, where we reuse those parameters for the optimized model. These functions also return accuracy,Precision, MAE,RSME and MAPE for each split.\n",
    "we used 10 split cross validation with 80% for training and 20% for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error\n",
    "def test_accuracy(model, n_splits=10, print_steps=False, params={}):\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    MAEs = []\n",
    "    rsmes = []\n",
    "    MAPEs = []\n",
    "    for i in range(1, n_splits+1):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=.2, random_state=i)\n",
    "        yhat, _ = model(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            **params\n",
    "        )\n",
    "        accuracy = float(sum(yhat==y_test)) / len(y_test)\n",
    "        accuracies.append(accuracy)\n",
    "        precision, recall, fscore, support = score(y_test, yhat)\n",
    "        precisions.append(precision)\n",
    "        MAE = float(mean_absolute_error(y_test, yhat))\n",
    "        MAEs.append(MAE)\n",
    "        rsme = float(np.sqrt(mean_squared_error(y_test, yhat)))\n",
    "        rsmes.append(rsme)\n",
    "        mask = y_test != 0\n",
    "        MAPE = float(np.fabs((y_test - yhat)/y_test)[mask].mean() * 100)\n",
    "        MAPEs.append(MAPE)\n",
    "        if print_steps:\n",
    "            matrix = pd.DataFrame(confusion_matrix(y_test, yhat),\n",
    "                columns=['Predicted 1', 'Predicted 0'],\n",
    "                index=['Actual 1', 'Actual 0'],\n",
    "            )\n",
    "            print('*' * 15 + ' Split %d ' % i + '*' * 15)\n",
    "            print('Precision:',precision[0])\n",
    "            print('Accuracy:', accuracy)\n",
    "            print(matrix)\n",
    "            print('-' * 40)\n",
    "            print('Cross Validation Fold Mean Error Scores')\n",
    "            print('MAE:',MAE)\n",
    "            print('RSME:',rsme)\n",
    "            print('MAPE:',MAPE)\n",
    "    Scores = [np.mean(accuracies),np.mean(precisions),np.mean(MAEs),np.mean(rsmes),np.mean(MAPEs)]\n",
    "    return Scores \n",
    "def find_optimal_accuracy(model, param, param_values, params={}):\n",
    "    result = {}\n",
    "    for param_value in tqdm(list(param_values)):\n",
    "        params_local = params.copy()\n",
    "        params_local[param] = param_value\n",
    "        result[param_value] = test_accuracy(model, params=params_local)\n",
    "    \n",
    "    result = pd.Series(result).sort_index()\n",
    "    plt.xlabel(param, fontsize=15)\n",
    "    plt.ylabel('Accuracy', fontsize=15)\n",
    "    \n",
    "    optimal_param = result.argmax()\n",
    "    optimal_accuracy = result[optimal_param]\n",
    "    \n",
    "    if type(param_value) == str:\n",
    "        result.plot(kind='bar')\n",
    "    else:\n",
    "        result.plot()\n",
    "    plt.show()\n",
    "    return optimal_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "For the logistic regression model, we created a function that took in X_train and Y_train from the original data set to test for X_test from the modified dataset. The accuracy of the logistic regression prediction for positive or negative cardio was compared with that of the original, where a confusion matrix was made to show percentage accuracy. Along with accuracy precision,MAE,RSME and MAPE for each split will be shown. Average Accuracy, Precision, MAE, RSME and MAPE for 10 folds cross validation is more than 0.72, 0.724, 0.279, 0.528 and 34.741 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Split 1 ***************\n",
      "Precision: 0.6938433617199498\n",
      "Accuracy: 0.7184527584020292\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         4970         1359\n",
      "Actual 0         2193         4094\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.28154724159797084\n",
      "RSME: 0.5306102539510247\n",
      "MAPE: 34.88150151105456\n",
      "*************** Split 2 ***************\n",
      "Precision: 0.7024655244463017\n",
      "Accuracy: 0.7201965757767914\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         5043         1394\n",
      "Actual 0         2136         4043\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.27980342422320864\n",
      "RSME: 0.5289644829506124\n",
      "MAPE: 34.56870043696391\n",
      "*************** Split 3 ***************\n",
      "Precision: 0.6929959457570251\n",
      "Accuracy: 0.7171052631578947\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         4957         1373\n",
      "Actual 0         2196         4090\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.28289473684210525\n",
      "RSME: 0.5318784981949405\n",
      "MAPE: 34.934775692014\n",
      "*************** Split 4 ***************\n",
      "Precision: 0.7079782031577476\n",
      "Accuracy: 0.7266169942929613\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         5067         1359\n",
      "Actual 0         2090         4100\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.27338300570703866\n",
      "RSME: 0.5228604074770231\n",
      "MAPE: 33.764135702746366\n",
      "*************** Split 5 ***************\n",
      "Precision: 0.701482199750658\n",
      "Accuracy: 0.7233671528218135\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         5064         1335\n",
      "Actual 0         2155         4062\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.2766328471781864\n",
      "RSME: 0.5259589786078249\n",
      "MAPE: 34.66302074955767\n",
      "*************** Split 6 ***************\n",
      "Precision: 0.7086873115922171\n",
      "Accuracy: 0.7243975903614458\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         5172         1351\n",
      "Actual 0         2126         3967\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.2756024096385542\n",
      "RSME: 0.5249784849291961\n",
      "MAPE: 34.89249958969309\n",
      "*************** Split 7 ***************\n",
      "Precision: 0.7012373140553315\n",
      "Accuracy: 0.7179771718452759\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         5044         1409\n",
      "Actual 0         2149         4014\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.2820228281547242\n",
      "RSME: 0.5310582154102544\n",
      "MAPE: 34.86938179458056\n",
      "*************** Split 8 ***************\n",
      "Precision: 0.6998614958448753\n",
      "Accuracy: 0.7220196575776792\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         5053         1340\n",
      "Actual 0         2167         4056\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.27798034242232084\n",
      "RSME: 0.5272384113684443\n",
      "MAPE: 34.82243291017194\n",
      "*************** Split 9 ***************\n",
      "Precision: 0.6958698372966208\n",
      "Accuracy: 0.7161540900443881\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         5004         1394\n",
      "Actual 0         2187         4031\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.28384590995561193\n",
      "RSME: 0.5327719117555015\n",
      "MAPE: 35.172081055001605\n",
      "*************** Split 10 ***************\n",
      "Precision: 0.6937439041382193\n",
      "Accuracy: 0.720434369055168\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         4979         1329\n",
      "Actual 0         2198         4110\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.27956563094483194\n",
      "RSME: 0.5287396627309435\n",
      "MAPE: 34.84464172479391\n",
      "--------------------------------------------------\n",
      "Average unoptimized accuracy: 0.720672\n",
      "Average unoptimized Precision: 0.724071\n",
      "Average unoptimized MAE: 0.279328\n",
      "Average unoptimized RSME: 0.528506\n",
      "Average unoptimized MAPE: 34.741317\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "def logistic_regression_model(X_train, y_train, X_test, **params):\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    clr = LogisticRegression(**params)\n",
    "    clr.fit(X_train, y_train)\n",
    "    return clr.predict(X_test), clr\n",
    "\n",
    "best_params_logistic = {}\n",
    "\n",
    "model = logistic_regression_model\n",
    "scores = test_accuracy(model=model, params=best_params_logistic, print_steps=True)\n",
    "print('-' * 50)\n",
    "print('Average unoptimized accuracy: %f' % scores[0])\n",
    "print('Average unoptimized Precision: %f' % scores[1])\n",
    "print('Average unoptimized MAE: %f' % scores[2])\n",
    "print('Average unoptimized RSME: %f' % scores[3])\n",
    "print('Average unoptimized MAPE: %f' % scores[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7206721623335446,\n",
       " 0.7240710236818055,\n",
       " 0.2793278376664553,\n",
       " 0.5285059307375766,\n",
       " 34.74131711665776]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SGDClassifier with loss = log we got average accuracy almost same as logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Split 1 ***************\n",
      "Precision: 0.6742532005689901\n",
      "Accuracy: 0.7119530754597336\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         5214         1115\n",
      "Actual 0         2519         3768\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.2880469245402663\n",
      "RSME: 0.5367000321783727\n",
      "MAPE: 40.06680451725783\n",
      "*************** Split 2 ***************\n",
      "Precision: 0.6678748338770086\n",
      "Accuracy: 0.7100507292327204\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         5528          909\n",
      "Actual 0         2749         3430\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.28994927076727967\n",
      "RSME: 0.5384693777433213\n",
      "MAPE: 44.48939957921994\n",
      "*************** Split 3 ***************\n",
      "Precision: 0.6952421171171171\n",
      "Accuracy: 0.718135700697527\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         4939         1391\n",
      "Actual 0         2165         4121\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.28186429930247303\n",
      "RSME: 0.5309089369208932\n",
      "MAPE: 34.44161629016863\n",
      "*************** Split 4 ***************\n",
      "Precision: 0.7145119493233516\n",
      "Accuracy: 0.726854787571338\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         4963         1463\n",
      "Actual 0         1983         4207\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.273145212428662\n",
      "RSME: 0.5226329614831636\n",
      "MAPE: 32.035541195476576\n",
      "*************** Split 5 ***************\n",
      "Precision: 0.702603800140746\n",
      "Accuracy: 0.7209892200380469\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         4992         1407\n",
      "Actual 0         2113         4104\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.27901077996195306\n",
      "RSME: 0.5282147100961436\n",
      "MAPE: 33.987453755830785\n",
      "*************** Split 6 ***************\n",
      "Precision: 0.6840604858701042\n",
      "Accuracy: 0.7183734939759037\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         5519         1004\n",
      "Actual 0         2549         3544\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.2816265060240964\n",
      "RSME: 0.5306849404534638\n",
      "MAPE: 41.83489249958969\n",
      "*************** Split 7 ***************\n",
      "Precision: 0.7069865913902611\n",
      "Accuracy: 0.7209892200380469\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         5009         1444\n",
      "Actual 0         2076         4087\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.27901077996195306\n",
      "RSME: 0.5282147100961436\n",
      "MAPE: 33.68489372059062\n",
      "*************** Split 8 ***************\n",
      "Precision: 0.6881989211945797\n",
      "Accuracy: 0.7200380469245403\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         5231         1162\n",
      "Actual 0         2370         3853\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.27996195307545974\n",
      "RSME: 0.5291143100271053\n",
      "MAPE: 38.084525148642136\n",
      "*************** Split 9 ***************\n",
      "Precision: 0.6820546163849155\n",
      "Accuracy: 0.7148065948002537\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         5245         1153\n",
      "Actual 0         2445         3773\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.28519340519974634\n",
      "RSME: 0.5340350224467927\n",
      "MAPE: 39.32132518494693\n",
      "*************** Split 10 ***************\n",
      "Precision: 0.702211230724469\n",
      "Accuracy: 0.7203551046290425\n",
      "          Predicted 1  Predicted 0\n",
      "Actual 1         4827         1481\n",
      "Actual 0         2047         4261\n",
      "----------------------------------------\n",
      "Cross Validation Fold Mean Error Scores\n",
      "MAE: 0.2796448953709575\n",
      "RSME: 0.5288146134241729\n",
      "MAPE: 32.450856055802156\n",
      "--------------------------------------------------\n",
      "Average unoptimized accuracy: 0.718255\n",
      "Average unoptimized Precision: 0.725447\n",
      "Average unoptimized MAE: 0.281745\n",
      "Average unoptimized RSME: 0.530779\n",
      "Average unoptimized MAPE: 37.039731\n"
     ]
    }
   ],
   "source": [
    "def SGDClassifier_log_model(X_train, y_train, X_test, **params):\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    params['loss'] = 'log'\n",
    "\n",
    "    clf = SGDClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.predict(X_test), clf\n",
    "\n",
    "best_params_logistic = {}\n",
    "\n",
    "model = SGDClassifier_log_model\n",
    "scores = test_accuracy(model=model, params=best_params_logistic, print_steps=True)\n",
    "print('-' * 50)\n",
    "print('Average unoptimized accuracy: %f' % scores[0])\n",
    "print('Average unoptimized Precision: %f' % scores[1])\n",
    "print('Average unoptimized MAE: %f' % scores[2])\n",
    "print('Average unoptimized RSME: %f' % scores[3])\n",
    "print('Average unoptimized MAPE: %f' % scores[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Logistic Regression Model\n",
    "By running logistic regression  one time with the built in parameters for both LogisticRegression and SGDClassifier, we got an average accuracy of 0.71 from 10 splits. To try to improve this, we are doing following steps.\n",
    "\n",
    "first, for LogisticRegression function we want to see how changing the value of C, class_weight, random_state, max_iter and penalty will affect the accuracy. To do this we are using GridSearchCV function to check 'C' value between 0.001, 0.01, 0.1, 1, 10, 100 and 1000. Also sets penalty as L2 and 'class_weight'to 'balanced' and 'none'. we also assign 0 and 'lbfgs' for random_state and solver respectively. max_iter will be defined between 1500 and 2000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 56 candidates, totalling 560 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:   31.2s\n",
      "[Parallel(n_jobs=8)]: Done 560 out of 560 | elapsed:   39.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=0, test_size=0.2, train_size=None),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='warn',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=8,\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
       "                         'class_weight': ['balanced', None],\n",
       "                         'max_iter': [1500, 2000], 'penalty': ['l1', 'l2'],\n",
       "                         'random_state': [0]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Divide data into test and training splits and having 10-fold CV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=0)\n",
    "#Logisitic regression 10-fold cross-validation \n",
    "\n",
    "regEstimator = LogisticRegression()\n",
    "\n",
    "parameters = { 'penalty':['l1','l2']\n",
    "              ,'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "              ,'class_weight': ['balanced',None]\n",
    "              ,'random_state': [0]\n",
    "              ,'max_iter':[1500,2000]\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the above parameters \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=regEstimator\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring='accuracy')\n",
    "\n",
    "#data scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "\n",
    "#Transform training data to z-scores\n",
    "#This makes our model's coefficients take on the same scale for accurate feature importance analysis \n",
    "X_Scl = scaler.transform(df)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X_Scl, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=1500, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the best estimator parameters\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy for all cv folds is: \t\t\t 0.71914\n",
      "The average precision for all cv folds is: \t\t\t 0.74094\n",
      "*********************************************************\n",
      "Cross Validation Fold Mean Error Scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.717898</td>\n",
       "      <td>0.750314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.715996</td>\n",
       "      <td>0.741457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.722654</td>\n",
       "      <td>0.732837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.715679</td>\n",
       "      <td>0.740146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.726141</td>\n",
       "      <td>0.743292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.720989</td>\n",
       "      <td>0.747712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.716630</td>\n",
       "      <td>0.734184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.717977</td>\n",
       "      <td>0.740701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.716392</td>\n",
       "      <td>0.744102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.721068</td>\n",
       "      <td>0.734642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision\n",
       "0  0.717898   0.750314\n",
       "1  0.715996   0.741457\n",
       "2  0.722654   0.732837\n",
       "3  0.715679   0.740146\n",
       "4  0.726141   0.743292\n",
       "5  0.720989   0.747712\n",
       "6  0.716630   0.734184\n",
       "7  0.717977   0.740701\n",
       "8  0.716392   0.744102\n",
       "9  0.721068   0.734642"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the best parameters for our Logistic Regression object\n",
    "classifierEst = regGridSearch.best_estimator_\n",
    "\n",
    "#Classifier Evaluation\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def EvaluateClassifierEstimator(classifierEstimator, X, Y, cv):\n",
    "   \n",
    "    #Perform cross validation \n",
    "    scores = cross_validate(classifierEstimator, df, y, scoring=['accuracy','precision']\n",
    "                            , cv=cv, return_train_score=True)\n",
    "\n",
    "    Accavg = scores['test_accuracy'].mean()\n",
    "    Preavg = scores['test_precision'].mean()\n",
    "\n",
    "    print_str = \"The average accuracy for all cv folds is: \\t\\t\\t {Accavg:.5}\"\n",
    "    print_str2 = \"The average precision for all cv folds is: \\t\\t\\t {Preavg:.5}\"\n",
    "\n",
    "\n",
    "    print(print_str.format(Accavg=Accavg))\n",
    "    print(print_str2.format(Preavg=Preavg))\n",
    "    print('*********************************************************')\n",
    "\n",
    "    print('Cross Validation Fold Mean Error Scores')\n",
    "    scoresResults = pd.DataFrame()\n",
    "    scoresResults['Accuracy'] = scores['test_accuracy']\n",
    "    scoresResults['Precision'] = scores['test_precision']\n",
    "\n",
    "    return scoresResults\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "EvaluateClassifierEstimator(classifierEst, X_Scl, y, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we want to see how changing the value of alpha, epsilon, number of iterations, and penalty will affect the accuracy for SGDClassifier model. To do this we have another 'For' loop which sets alpha and epsilon at ten and twenty linear increments from 0.00001 to 0.001 and 0.01 to .5, respectively. The number of iterations could be 1, 3, 6, 10, or 15 and penalty could be L1 or L2.\n",
    "\n",
    "The optimal values will be stored at best_params_logistic variable and will used to fit for final model.\n",
    "\n",
    "The optimal value for alpha we found is 0.00034 and that for epsilon is 0.371. The optimal penalty is L2 at 15 iterations.Alpha is just a constant multiplied to the regularization term so our value of 0.00034 is expected.\n",
    "\n",
    "We found L2, the squared error, is slightly more accurate than L1, the error. This was expected because L2 is typically better for minimizing error than L1 and L2 is standard for linear SVM models, where it performed the best for our model.\n",
    "\n",
    "The iteration number vs accuracy should be a fairly random distribution. We expected to get different results each time and expected that they would be about our initial accuracy, 71%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-d9d6a0671bb3>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-d9d6a0671bb3>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    SGDClassifier_log_model,\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "test_params = [\n",
    "    ('n_iter_no_change', [1, 3, 6, 10, 15]),\n",
    "    ('alpha', np.linspace(0.00001, 0.001, 10)),\n",
    "    ('epsilon', np.linspace(0.01, .5, 20)),\n",
    "    ('penalty', ['l1', 'l2'])\n",
    "]\n",
    "\n",
    "for param, param_values in test_params:\n",
    "    best_params_logistic[param] = find_optimal_accuracy(\n",
    "        SGDClassifier_log_model,\n",
    "        param=param,\n",
    "        param_values=param_values,\n",
    "        params=best_params_logistic\n",
    "    )\n",
    "    print(\"Best\", param, best_params_logistic[param])\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Logistic Regression Model Performance\n",
    "Once we plugged in all optimal values into the model, the final accuracy became 0.719, which is slightly better than that of 0.718 from default parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "scores = test_accuracy(\n",
    "    SGDClassifier_log_model, n_splits=10, params=best_params_logistic)\n",
    "print('Optimized Logistic Regression Accuracy: %f' % scores[0])\n",
    "print('Optimized Logistic Regression Precision: %f' % scores[1])\n",
    "print('Optimized Logistic Regression MAE: %f' % scores[2])\n",
    "print('Optimized Logistic Regressiond RSME: %f' % scores[3])\n",
    "print('Optimized Logistic Regression MAPE: %f' % scores[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Model\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=.2, random_state=1)\n",
    "scaler = StandardScaler()\n",
    "X_train_lr = scaler.fit_transform(X_train)\n",
    "X_test_lr = scaler.transform(X_test)\n",
    "Best_clf = SGDClassifier(loss='log',**best_params_logistic)\n",
    "Best_clf.fit(X_train_lr, y_train)\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(Best_clf.coef_.T,df.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weight\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "weights = Best_clf.coef_[0]\n",
    "feature_names = df.columns.values\n",
    "linreg_ft_imp_df = pd.DataFrame({'feature_names':feature_names, 'weights':weights, 'absolute_weights': np.abs(weights)})\n",
    "linreg_ft_imp_df.sort_values(by='absolute_weights', inplace=True, ascending=False )\n",
    "#drawing the coefficients with SNS\n",
    "import seaborn as sns\n",
    "ax = sns.barplot(x =linreg_ft_imp_df['weights'], y = linreg_ft_imp_df['feature_names'], orient= 'h')\n",
    "ax.set_title(\"Top Feature Correlations\")\n",
    "ax.set_xlabel(\"Coefficient Magnitude\\n(z-score)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar plots above will help interpret the importance of different features for the classification task.\n",
    "\n",
    "General Observation - It is obvious that Systolic blood pressure (ap_hi) holds the most weight in our prediction. There is a sharp drop to the second and third coefficients; their age in years and the patents cholesterol. The last two positive correlated features are weight and Diastolic blood pressure (ap_lo), theses seem to play less of a roll in the prediction of cardiovascular diseases. Looking at the graph these features fill out the positive correlation, meaning the higher the numbers the greater the risk of having some form of Cardiovascular diseases. This makes since looking at the features in this group. The higher someoneâ€™s blood pressure, age or cholesterol the greater the risk. We can now look at the most important negative correlated feature, physical activity. The lower a personâ€™s physical activity level (active) is the more of a predictor of Cardiovascular diseases.\n",
    "\n",
    "ap_hi â€“ This is the highest influential factor in our analysis. We saw a strong linear relationship between this variable and cardiovascular diseases (cardio). The rate of cardiovascular diseases tends to rise Systolic blood pressure increases.\n",
    "\n",
    "years â€“ The age of a patent was the second in the prediction of cardo. This would also make since as that the older a person is the more risk of developing cardiovascular problems. We found that the data showed a person in their mid-50â€™s and older was more at risk.\n",
    "\n",
    "cholesterol â€“ Cholesterol and years were almost equal in weight meaning the higher the cholesterol the more of a person is at risk of cardiovascular diseases.\n",
    "\n",
    "weight â€“ Weight was one of the lower predictors but still showed to have slight significances.\n",
    "\n",
    "ap_lo â€“ One surprising feature to show not much in the predictions was a personâ€™s Diastolic blood pressure (ap_lo). It was the lowest of the positive correlated features.\n",
    "\n",
    "active - The last feature we would like to point out is the physical activity of the patents. There is significant negative correlation in the prediction. This also was not any new revelation, it basically points out that the less active a person is the more of a predictor of cardiovascular diseases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.SVM\n",
    "For the support vector machine model, we created a function that took in X_train and Y_train from the original data set to test for X_test from the modified dataset. The accuracy of the SVM prediction for positive or negative cardio was compared with that of the original, where a confusion matrix was made to show percentage accuracy. Due to the complexity of the dataset, we are again slightly better than 71% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_machine_model(X_train, y_train, X_test, **params):\n",
    "    \n",
    "    # X = (X - Âµ) / Ïƒ\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    params['loss'] = 'hinge'\n",
    "    clf = SGDClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.predict(X_test), clf\n",
    "\n",
    "best_params_svc = {}\n",
    "\n",
    "\n",
    "model = support_vector_machine_model\n",
    "score = test_accuracy(model=model, params=best_params_svc, print_steps=True)\n",
    "print('-' * 50)\n",
    "print('Average unoptimized accuracy: %f' % scores[0])\n",
    "print('Average unoptimized Precision: %f' % scores[1])\n",
    "print('Average unoptimized MAE: %f' % scores[2])\n",
    "print('Average unoptimized RSME: %f' % scores[3])\n",
    "print('Average unoptimized MAPE: %f' % scores[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Support Vector Machine Model\n",
    "By running SVM model one time with the built in parameters, we got an average accuracy of 0.71 from 10 splits. To try to improve this, we will do a few things listed below:\n",
    "\n",
    "First, we want to do the 80/20 split 10 times and average those results to get a better accuracy. By splitting the training and test sets up multiple times, we can minimize the effects of outliers.\n",
    "\n",
    "Second, we want to see how changing the value of alpha,epsilon, number of iterations, and penalty will affect the accuracy. To do this we have another for loop which sets alpha and epsilon at 10 and 20 linear increments from 0.00001 to 0.01 and 0.01 to 0.5 respectively. The number of iterations could be 10, 15, 30, 60, or 100 and penalty could be L1 or L2.\n",
    "\n",
    "We found that the optimal value for alpha is 0.00421. The optimal penalty is L2 at 100 iterations.\n",
    "\n",
    "Alpha is just a constant multiplied to the regularization term so our value of 0.00421 is expected. Alpha could be used again if we set the learning rate to optimal but we will not do that for this mini lab.\n",
    "\n",
    "Epsilon was not changed because the results had noisy accuracy and we decided to remove it.\n",
    "\n",
    "We found L2, the squared error, is slightly more accurate than L1, the error. This was expected because L2 is typically better for minimizing error than L1 and L2 is standard for linear SVM models, where it performed the best for our model.\n",
    "\n",
    "The iteration number vs accuracy should be a fairly random distribution. We expected to get different results each time and expected that they would be about our initial accuracy, 0.53 +/- 0.1. This time, 100 iterations is the optimal number. Although the accuracy per iteration was still going up, we had to stop at 100 iterations due to running time restraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = [\n",
    "    ('n_iter_no_change', [10, 15, 30, 60, 100]),\n",
    "    ('alpha', np.linspace(0.00001, 0.001, 10)),\n",
    "    ('epsilon', np.linspace(0.01, .5, 20)),\n",
    "    ('penalty', ['l1', 'l2'])]\n",
    "    \n",
    "model = support_vector_machine_model\n",
    "\n",
    "for param, test_values in test_params:\n",
    "    best_params_svc[param] = find_optimal_accuracy(\n",
    "        model=model,\n",
    "        param=param,\n",
    "        param_values=test_values,\n",
    "        params=best_params_svc\n",
    "    )\n",
    "    print(\"Best\", param, best_params_svc[param])\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Model\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=.2, random_state=1)\n",
    "scaler = StandardScaler()\n",
    "X_train_lr = scaler.fit_transform(X_train)\n",
    "X_test_lr = scaler.transform(X_test)\n",
    "Best_clf = SGDClassifier(loss='hinge',**best_params_svc)\n",
    "Best_clf.fit(X_train_lr, y_train)\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(Best_clf.coef_.T,df.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weight\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(Best_clf.coef_[0],index=df.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "clf.fit(X_scaled, y)\n",
    "\n",
    "# this hold the indexes of the support vectors\n",
    "clf.support_\n",
    "\n",
    "# this holds a subset of the data which is used for support vectors\n",
    "support_vectors = pd.DataFrame(clf.support_vectors_, columns=df.columns)\n",
    "\n",
    "# get number of support vectors for each class\n",
    "print('Number of support vectors for each feature:', clf.n_support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_grouped = support_vectors.groupby(y.loc[clf.support_].values)\n",
    "X_grouped = df.groupby(y.values)\n",
    "\n",
    "vars_to_plot = ['years','ap_hi','ap_lo','height','weight','cholesterol','gluc','BMI']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4)).subplots_adjust(wspace=.4)\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    V_grouped[v].plot.kde() \n",
    "    plt.legend(['cardio 0','cardio 1'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    X_grouped[v].plot.kde() \n",
    "    plt.legend(['cardio 0','cardio 1'])\n",
    "    plt.title(v+' (Original)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
